[1/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/general_kernels.cu -o general_kernels.cuda.o 
[2/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels_new.cu -o transform_kernels_new.cuda.o 
[3/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cublas_wrappers.cu -o cublas_wrappers.cuda.o 
[4/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/embedding_kernels.cu -o embedding_kernels.cuda.o 
[5/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels.cu -o transform_kernels.cuda.o 
[6/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/normalize_kernels.cu -o normalize_kernels.cuda.o 
[7/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu -o dropout_kernels.cuda.o 
/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1509): warning: variable "thread_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1513): warning: variable "temp_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1545): warning: variable "block_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kRelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kRelu, T=float]" 
(1754): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kGelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kGelu, T=float]" 
(1770): here

[8/38] c++ -MMD -MF layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/layer.cpp -o layer.o 
[9/38] c++ -MMD -MF context.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/context.cpp -o context.o 
[10/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/crf.cu -o crf.cuda.o 
[11/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels_new.cu -o softmax_kernels_new.cuda.o 
[12/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels.cu -o softmax_kernels.cuda.o 
[13/38] c++ -MMD -MF node.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/node.cpp -o node.o 
[14/38] c++ -MMD -MF manager.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/manager.cpp -o manager.o 
[15/38] c++ -MMD -MF tensor.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/tensor.cpp -o tensor.o 
[16/38] c++ -MMD -MF bias_act_dropout.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/bias_act_dropout.cpp -o bias_act_dropout.o 
[17/38] c++ -MMD -MF bias_dropout_residual.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/bias_dropout_residual.cpp -o bias_dropout_residual.o 
[18/38] c++ -MMD -MF linear.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/linear.cpp -o linear.o 
[19/38] c++ -MMD -MF bias_add_transform_20314.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/bias_add_transform_20314.cpp -o bias_add_transform_20314.o 
[20/38] c++ -MMD -MF layer_normalize.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/layer_normalize.cpp -o layer_normalize.o 
[21/38] c++ -MMD -MF strided_batch_gemm.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/strided_batch_gemm.cpp -o strided_batch_gemm.o 
[22/38] c++ -MMD -MF softmax.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/softmax.cpp -o softmax.o 
[23/38] c++ -MMD -MF dropout.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/dropout.cpp -o dropout.o 
[24/38] c++ -MMD -MF launch_concat3_dim1.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/launch_concat3_dim1.cpp -o launch_concat3_dim1.o 
[25/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cross_entropy.cu -o cross_entropy.cuda.o 
[26/38] c++ -MMD -MF transform_0213.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/transform_0213.cpp -o transform_0213.o 
[27/38] c++ -MMD -MF crf.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/crf.cpp -o crf.o 
[28/38] c++ -MMD -MF transformer_encoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/transformer_encoder_layer.cpp -o transformer_encoder_layer.o 
[29/38] c++ -MMD -MF feed_forward_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/feed_forward_layer.cpp -o feed_forward_layer.o 
[30/38] c++ -MMD -MF encdec_kv_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/encdec_kv_layer.cpp -o encdec_kv_layer.o 
[31/38] c++ -MMD -MF multihead_attention_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/multihead_attention_layer.cpp -o multihead_attention_layer.o 
[32/38] c++ -MMD -MF transformer_decoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/transformer_decoder_layer.cpp -o transformer_decoder_layer.o 
[33/38] c++ -MMD -MF dec_enc_attention_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/dec_enc_attention_layer.cpp -o dec_enc_attention_layer.o 
[34/38] c++ -MMD -MF dec_self_attention_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/dec_self_attention_layer.cpp -o dec_self_attention_layer.o 
[35/38] c++ -MMD -MF crf_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/crf_layer.cpp -o crf_layer.o 
[36/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cuda_util.cu -o cuda_util.cuda.o 
[37/38] c++ -MMD -MF pybind_layer_new.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_layer_new.cpp -o pybind_layer_new.o 
[38/38] c++ cublas_wrappers.cuda.o transform_kernels.cuda.o transform_kernels_new.cuda.o dropout_kernels.cuda.o normalize_kernels.cuda.o softmax_kernels.cuda.o softmax_kernels_new.cuda.o general_kernels.cuda.o cuda_util.cuda.o embedding_kernels.cuda.o cross_entropy.cuda.o crf.cuda.o context.o layer.o manager.o node.o tensor.o bias_act_dropout.o bias_dropout_residual.o linear.o layer_normalize.o strided_batch_gemm.o bias_add_transform_20314.o dropout.o softmax.o launch_concat3_dim1.o transform_0213.o crf.o feed_forward_layer.o multihead_attention_layer.o transformer_encoder_layer.o dec_self_attention_layer.o encdec_kv_layer.o dec_enc_attention_layer.o transformer_decoder_layer.o crf_layer.o pybind_layer_new.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o lightseq_layers_new.so
Time to load lightseq_layers_new op: 51.66050410270691 seconds
[1/18] c++ -MMD -MF cublas_wrappers.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cublas_wrappers.cpp -o cublas_wrappers.o 
[2/18] c++ -MMD -MF cublas_algo_map.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cublas_algo_map.cpp -o cublas_algo_map.o 
[3/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/general_kernels.cu -o general_kernels.cuda.o 
[4/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/quantize_kernels.cu -o quantize_kernels.cuda.o 
[5/18] c++ -MMD -MF cross_entropy_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/cross_entropy_layer.cpp -o cross_entropy_layer.o 
[6/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels.cu -o transform_kernels.cuda.o 
[7/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/embedding_kernels.cu -o embedding_kernels.cuda.o 
[8/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/normalize_kernels.cu -o normalize_kernels.cuda.o 
[9/18] c++ -MMD -MF quant_linear_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/quant_linear_layer.cpp -o quant_linear_layer.o 
[10/18] c++ -MMD -MF transformer_embedding_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/transformer_embedding_layer.cpp -o transformer_embedding_layer.o 
[11/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu -o dropout_kernels.cuda.o 
/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1509): warning: variable "thread_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1513): warning: variable "temp_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1545): warning: variable "block_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kRelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kRelu, T=float]" 
(1754): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kGelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kGelu, T=float]" 
(1770): here

[12/18] c++ -MMD -MF transformer_encoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/transformer_encoder_layer.cpp -o transformer_encoder_layer.o 
[13/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels.cu -o softmax_kernels.cuda.o 
[14/18] c++ -MMD -MF transformer_decoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/transformer_decoder_layer.cpp -o transformer_decoder_layer.o 
[15/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cross_entropy.cu -o cross_entropy.cuda.o 
[16/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cuda_util.cu -o cuda_util.cuda.o 
[17/18] c++ -MMD -MF pybind_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_layer.cpp -o pybind_layer.o 
[18/18] c++ cublas_algo_map.o cublas_wrappers.o quantize_kernels.cuda.o transform_kernels.cuda.o dropout_kernels.cuda.o normalize_kernels.cuda.o softmax_kernels.cuda.o general_kernels.cuda.o cuda_util.cuda.o embedding_kernels.cuda.o cross_entropy.cuda.o cross_entropy_layer.o quant_linear_layer.o transformer_encoder_layer.o transformer_decoder_layer.o transformer_embedding_layer.o pybind_layer.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o lightseq_layers.so
Time to load lightseq_layers op: 40.187997341156006 seconds
Time to load lightseq_layers op: 0.035695791244506836 seconds
Time to load lightseq_layers op: 0.03760242462158203 seconds
Time to load lightseq_layers op: 0.03436636924743652 seconds
[1/14] c++ -MMD -MF gemm_test.o.d -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/gemm_test.cpp -o gemm_test.o 
[2/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/general_kernels.cu -o general_kernels.cuda.o 
[3/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels.cu -o transform_kernels.cuda.o 
[4/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels_new.cu -o transform_kernels_new.cuda.o 
[5/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/embedding_kernels.cu -o embedding_kernels.cuda.o 
[6/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/normalize_kernels.cu -o normalize_kernels.cuda.o 
[7/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/crf.cu -o crf.cuda.o 
[8/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu -o dropout_kernels.cuda.o 
/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1509): warning: variable "thread_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1513): warning: variable "temp_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1545): warning: variable "block_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kRelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kRelu, T=float]" 
(1754): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kGelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kGelu, T=float]" 
(1770): here

[9/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/quantize_kernels.cu -o quantize_kernels.cuda.o 
[10/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels.cu -o softmax_kernels.cuda.o 
[11/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels_new.cu -o softmax_kernels_new.cuda.o 
[12/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cuda_util.cu -o cuda_util.cuda.o 
[13/14] c++ -MMD -MF pybind_kernel.o.d -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_kernel.cpp -o pybind_kernel.o 
[14/14] c++ gemm_test.o cuda_util.cuda.o transform_kernels.cuda.o transform_kernels_new.cuda.o softmax_kernels.cuda.o softmax_kernels_new.cuda.o general_kernels.cuda.o normalize_kernels.cuda.o dropout_kernels.cuda.o embedding_kernels.cuda.o quantize_kernels.cuda.o crf.cuda.o pybind_kernel.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o lightseq_kernels.so
Time to load lightseq_kernels op: 39.6067590713501 seconds
2024-12-12 05:13:04 | INFO | fairseq_cli.train | Namespace(GCQ_quantile=0.99, activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='ls_transformer_wmt_en_de_big_t2t', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='ls_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/tmp/wmt14_en_de/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=12, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_GCQ=False, enable_quant=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='simple', log_interval=10, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=300, max_target_positions=300, max_tokens=512, max_tokens_valid=512, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_lr=-1.0, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='ls_adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_mode='qat', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0.05, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, use_torch_layer=False, user_dir='/opt/conda/lib/python3.7/site-packages/lightseq/training/cli/fs_modules', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2024-12-12 05:13:05 | INFO | fairseq.tasks.translation | [en] dictionary: 40480 types
2024-12-12 05:13:05 | INFO | fairseq.tasks.translation | [de] dictionary: 42720 types
2024-12-12 05:13:05 | INFO | fairseq.data.data_utils | loaded 39414 examples from: /tmp/wmt14_en_de/valid.en-de.en
2024-12-12 05:13:05 | INFO | fairseq.data.data_utils | loaded 39414 examples from: /tmp/wmt14_en_de/valid.en-de.de
2024-12-12 05:13:05 | INFO | fairseq.tasks.translation | /tmp/wmt14_en_de/ valid en-de 39414 examples
Initial Context, status_type: Training
Embedding layer #0 is created with date type [half].
Embedding layer #1 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #0 allocate shared memory size: 5079040
Encoder layer #0 allocate shared quant memory size: 6815744
Encoder layer #0 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #1 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #2 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #3 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #4 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #5 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #6 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #7 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #8 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #9 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #10 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #11 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #0 allocate shared memory size: 5079040
Decoder layer #0 allocate shared quant memory size: 6815744
Decoder layer #0 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #1 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #2 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #3 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #4 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #5 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #6 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #7 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #8 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #9 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #10 is created with date type [half].
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 0}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 1}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 2}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 3}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 4}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 5}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 6}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 7}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 8}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 9}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 10}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 11}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 0}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 1}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 2}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 3}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 4}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 5}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 6}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 7}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 8}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 9}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 10}
Lightseq Transformer config is  {'max_batch_tokens': 512, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 12, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 11}Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #11 is created with date type [half].
QuantLinearLayer is created with date type [half].
CrossEntropyLayer is created with date type [half].

Time to load lightseq_layers op: 0.04663443565368652 seconds
Time to load lightseq_layers op: 0.03976941108703613 seconds
2024-12-12 05:13:08 | INFO | fairseq_cli.train | LSTransformerModel(
  (encoder): LSTransformerEncoder(
    (embed_tokens): LSTransformerEmbeddingLayer()
    (layers): ModuleList(
      (0): LSTransformerEncoderLayer()
      (1): LSTransformerEncoderLayer()
      (2): LSTransformerEncoderLayer()
      (3): LSTransformerEncoderLayer()
      (4): LSTransformerEncoderLayer()
      (5): LSTransformerEncoderLayer()
      (6): LSTransformerEncoderLayer()
      (7): LSTransformerEncoderLayer()
      (8): LSTransformerEncoderLayer()
      (9): LSTransformerEncoderLayer()
      (10): LSTransformerEncoderLayer()
      (11): LSTransformerEncoderLayer()
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): LSTransformerDecoder(
    (embed_tokens): LSTransformerEmbeddingLayer()
    (layers): ModuleList(
      (0): LSFSTransformerDecoderLayer()
      (1): LSFSTransformerDecoderLayer()
      (2): LSFSTransformerDecoderLayer()
      (3): LSFSTransformerDecoderLayer()
      (4): LSFSTransformerDecoderLayer()
      (5): LSFSTransformerDecoderLayer()
      (6): LSFSTransformerDecoderLayer()
      (7): LSFSTransformerDecoderLayer()
      (8): LSFSTransformerDecoderLayer()
      (9): LSFSTransformerDecoderLayer()
      (10): LSFSTransformerDecoderLayer()
      (11): LSFSTransformerDecoderLayer()
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): LSQuantLinearLayer()
  )
)
2024-12-12 05:13:08 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2024-12-12 05:13:08 | INFO | fairseq_cli.train | model: ls_transformer_wmt_en_de_big_t2t (LSTransformerModel)
2024-12-12 05:13:08 | INFO | fairseq_cli.train | criterion: ls_label_smoothed_cross_entropy (LSLabelSmoothedCrossEntropyCriterion)
2024-12-12 05:13:08 | INFO | fairseq_cli.train | num. model params: 437916082 (num. trained: 437916082)
2024-12-12 05:13:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-12 05:13:08 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-SXM2-16GB                    
2024-12-12 05:13:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-12 05:13:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-12-12 05:13:08 | INFO | fairseq_cli.train | max tokens per GPU = 512 and max sentences per GPU = None
2024-12-12 05:13:08 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2024-12-12 05:13:08 | INFO | fairseq.trainer | loading train data for epoch 1
2024-12-12 05:13:08 | INFO | fairseq.data.data_utils | loaded 3900502 examples from: /tmp/wmt14_en_de/train.en-de.en
2024-12-12 05:13:08 | INFO | fairseq.data.data_utils | loaded 3900502 examples from: /tmp/wmt14_en_de/train.en-de.de
2024-12-12 05:13:08 | INFO | fairseq.tasks.translation | /tmp/wmt14_en_de/ train en-de 3900502 examples
2024-12-12 05:13:13 | INFO | fs_modules.ls_adam | using LightSeq Adam
[1/3] c++ -MMD -MF pybind_adam.o.d -DTORCH_EXTENSION_NAME=adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_adam.cpp -o pybind_adam.o 
[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/fused_adam_kernel.cu -o fused_adam_kernel.cuda.o 
[3/3] c++ fused_adam_kernel.cuda.o pybind_adam.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o adam.so
Time to load adam op: 55.22310948371887 seconds
2024-12-12 05:14:08 | INFO | fairseq.trainer | begin training epoch 1
TransformerEmbeddingLayer #0 bind weights and grads.
TransformerEncoderLayer #0 bind weights and grads.
TransformerEncoderLayer #1 bind weights and grads.
TransformerEncoderLayer #2 bind weights and grads.
TransformerEncoderLayer #3 bind weights and grads.
TransformerEncoderLayer #4 bind weights and grads.
TransformerEncoderLayer #5 bind weights and grads.
TransformerEncoderLayer #6 bind weights and grads.
TransformerEncoderLayer #7 bind weights and grads.
TransformerEncoderLayer #8 bind weights and grads.
TransformerEncoderLayer #9 bind weights and grads.
TransformerEncoderLayer #10 bind weights and grads.
TransformerEncoderLayer #11 bind weights and grads.
TransformerEmbeddingLayer #1 bind weights and grads.
TransformerDecoderLayer #0 bind weights and grads.
Decoder layer #0 allocate encdec_kv memory
TransformerDecoderLayer #1 bind weights and grads.
TransformerDecoderLayer #2 bind weights and grads.
TransformerDecoderLayer #3 bind weights and grads.
TransformerDecoderLayer #4 bind weights and grads.
TransformerDecoderLayer #5 bind weights and grads.
TransformerDecoderLayer #6 bind weights and grads.
TransformerDecoderLayer #7 bind weights and grads.
TransformerDecoderLayer #8 bind weights and grads.
TransformerDecoderLayer #9 bind weights and grads.
TransformerDecoderLayer #10 bind weights and grads.
TransformerDecoderLayer #11 bind weights and grads.
2024-12-12 05:14:09 | INFO | train_inner | epoch 001:     10 / 312876 loss=21.369, nll_loss=21.36, ppl=2.69241e+06, wps=5329.3, ups=16.97, wpb=330.4, bsz=12, num_updates=10, lr=1.25e-06, gnorm=18.653, loss_scale=128, train_wall=1, wall=60
2024-12-12 05:14:09 | INFO | train_inner | epoch 001:     20 / 312876 loss=21.303, nll_loss=21.291, ppl=2.56546e+06, wps=6173.8, ups=17.88, wpb=345.3, bsz=7.4, num_updates=20, lr=2.5e-06, gnorm=20.055, loss_scale=128, train_wall=1, wall=61
2024-12-12 05:14:10 | INFO | train_inner | epoch 001:     30 / 312876 loss=19.139, nll_loss=19.128, ppl=572955, wps=7038.8, ups=17.51, wpb=401.9, bsz=11.7, num_updates=30, lr=3.75e-06, gnorm=17.035, loss_scale=128, train_wall=1, wall=62
2024-12-12 05:14:10 | INFO | train_inner | epoch 001:     40 / 312876 loss=16.913, nll_loss=16.891, ppl=121543, wps=6664.2, ups=17.72, wpb=376, bsz=13.6, num_updates=40, lr=5e-06, gnorm=8.789, loss_scale=128, train_wall=1, wall=62
2024-12-12 05:14:11 | INFO | train_inner | epoch 001:     50 / 312876 loss=15.958, nll_loss=15.909, ppl=61540.2, wps=7063.3, ups=17.27, wpb=408.9, bsz=16.5, num_updates=50, lr=6.25e-06, gnorm=4.985, loss_scale=128, train_wall=1, wall=63
2024-12-12 05:14:12 | INFO | train_inner | epoch 001:     60 / 312876 loss=15.43, nll_loss=15.334, ppl=41316.5, wps=6843.9, ups=17.4, wpb=393.4, bsz=12.2, num_updates=60, lr=7.5e-06, gnorm=4.244, loss_scale=128, train_wall=1, wall=63
2024-12-12 05:14:12 | INFO | train_inner | epoch 001:     70 / 312876 loss=15.003, nll_loss=14.857, ppl=29683.1, wps=5990.4, ups=17.13, wpb=349.6, bsz=11.8, num_updates=70, lr=8.75e-06, gnorm=3.788, loss_scale=128, train_wall=1, wall=64
2024-12-12 05:14:13 | INFO | train_inner | epoch 001:     80 / 312876 loss=14.593, nll_loss=14.395, ppl=21537, wps=6047.6, ups=17.67, wpb=342.3, bsz=10.1, num_updates=80, lr=1e-05, gnorm=3.418, loss_scale=128, train_wall=1, wall=64
2024-12-12 05:14:13 | INFO | train_inner | epoch 001:     90 / 312876 loss=14.282, nll_loss=14.047, ppl=16932.2, wps=6743.7, ups=17.77, wpb=379.6, bsz=17.4, num_updates=90, lr=1.125e-05, gnorm=3.186, loss_scale=128, train_wall=1, wall=65
2024-12-12 05:14:14 | INFO | train_inner | epoch 001:    100 / 312876 loss=14.091, nll_loss=13.837, ppl=14630.4, wps=6442.4, ups=17.67, wpb=364.6, bsz=13.5, num_updates=100, lr=1.25e-05, gnorm=2.856, loss_scale=128, train_wall=1, wall=66
2024-12-12 05:14:14 | INFO | train_inner | epoch 001:    110 / 312876 loss=13.805, nll_loss=13.522, ppl=11762.8, wps=6122, ups=16.74, wpb=365.6, bsz=12.8, num_updates=110, lr=1.375e-05, gnorm=2.612, loss_scale=128, train_wall=1, wall=66
2024-12-12 05:14:15 | INFO | train_inner | epoch 001:    120 / 312876 loss=13.607, nll_loss=13.304, ppl=10113.8, wps=7038.6, ups=16.83, wpb=418.2, bsz=10.3, num_updates=120, lr=1.5e-05, gnorm=2.431, loss_scale=128, train_wall=1, wall=67
2024-12-12 05:14:16 | INFO | train_inner | epoch 001:    130 / 312876 loss=13.45, nll_loss=13.13, ppl=8962.24, wps=6964.8, ups=17.35, wpb=401.5, bsz=8.5, num_updates=130, lr=1.625e-05, gnorm=2.265, loss_scale=128, train_wall=1, wall=67
2024-12-12 05:14:16 | INFO | train_inner | epoch 001:    140 / 312876 loss=13.177, nll_loss=12.826, ppl=7259.58, wps=6918.1, ups=17.47, wpb=396, bsz=14.4, num_updates=140, lr=1.75e-05, gnorm=2.329, loss_scale=128, train_wall=1, wall=68
2024-12-12 05:14:17 | INFO | train_inner | epoch 001:    150 / 312876 loss=13.04, nll_loss=12.67, ppl=6517.35, wps=6935.7, ups=17.46, wpb=397.2, bsz=14.6, num_updates=150, lr=1.875e-05, gnorm=2.387, loss_scale=128, train_wall=1, wall=68
2024-12-12 05:14:17 | INFO | train_inner | epoch 001:    160 / 312876 loss=13.08, nll_loss=12.714, ppl=6719.62, wps=7677.6, ups=17.59, wpb=436.5, bsz=12.3, num_updates=160, lr=2e-05, gnorm=2.088, loss_scale=128, train_wall=1, wall=69
2024-12-12 05:14:18 | INFO | train_inner | epoch 001:    170 / 312876 loss=12.755, nll_loss=12.349, ppl=5216.65, wps=6963.6, ups=17.27, wpb=403.2, bsz=10.4, num_updates=170, lr=2.125e-05, gnorm=2.081, loss_scale=128, train_wall=1, wall=70
2024-12-12 05:14:18 | INFO | train_inner | epoch 001:    180 / 312876 loss=12.786, nll_loss=12.378, ppl=5323.65, wps=6262, ups=18.18, wpb=344.5, bsz=8.7, num_updates=180, lr=2.25e-05, gnorm=2.152, loss_scale=128, train_wall=1, wall=70
2024-12-12 05:14:19 | INFO | train_inner | epoch 001:    190 / 312876 loss=12.602, nll_loss=12.171, ppl=4612.76, wps=6581.3, ups=17.99, wpb=365.9, bsz=9.8, num_updates=190, lr=2.375e-05, gnorm=2.179, loss_scale=128, train_wall=1, wall=71
2024-12-12 05:14:20 | INFO | train_inner | epoch 001:    200 / 312876 loss=12.478, nll_loss=12.026, ppl=4169.69, wps=6079.8, ups=18.04, wpb=337, bsz=8.5, num_updates=200, lr=2.5e-05, gnorm=2.147, loss_scale=128, train_wall=1, wall=71
2024-12-12 05:14:20 | INFO | train_inner | epoch 001:    210 / 312876 loss=12.483, nll_loss=12.028, ppl=4174.95, wps=6977.8, ups=17.25, wpb=404.4, bsz=13.2, num_updates=210, lr=2.625e-05, gnorm=2.131, loss_scale=128, train_wall=1, wall=72
2024-12-12 05:14:21 | INFO | train_inner | epoch 001:    220 / 312876 loss=12.278, nll_loss=11.791, ppl=3543.01, wps=5858.3, ups=17.23, wpb=340, bsz=12.8, num_updates=220, lr=2.75e-05, gnorm=2.171, loss_scale=128, train_wall=1, wall=72
2024-12-12 05:14:21 | INFO | train_inner | epoch 001:    230 / 312876 loss=12.406, nll_loss=11.928, ppl=3896.88, wps=6848.8, ups=17.47, wpb=392, bsz=8.4, num_updates=230, lr=2.875e-05, gnorm=1.942, loss_scale=128, train_wall=1, wall=73
2024-12-12 05:14:22 | INFO | train_inner | epoch 001:    240 / 312876 loss=12.186, nll_loss=11.674, ppl=3267.12, wps=6786, ups=17.14, wpb=396, bsz=14.4, num_updates=240, lr=3e-05, gnorm=2.073, loss_scale=128, train_wall=1, wall=74
2024-12-12 05:14:22 | INFO | train_inner | epoch 001:    250 / 312876 loss=11.828, nll_loss=11.269, ppl=2467.76, wps=6245.5, ups=17.58, wpb=355.2, bsz=17.6, num_updates=250, lr=3.125e-05, gnorm=2.267, loss_scale=128, train_wall=1, wall=74
2024-12-12 05:14:23 | INFO | train_inner | epoch 001:    260 / 312876 loss=12.015, nll_loss=11.466, ppl=2828.29, wps=7075.6, ups=17.48, wpb=404.8, bsz=16, num_updates=260, lr=3.25e-05, gnorm=2.095, loss_scale=128, train_wall=1, wall=75
2024-12-12 05:14:24 | INFO | train_inner | epoch 001:    270 / 312876 loss=12.023, nll_loss=11.473, ppl=2842.26, wps=7252.6, ups=17.95, wpb=404, bsz=14.4, num_updates=270, lr=3.375e-05, gnorm=1.965, loss_scale=128, train_wall=1, wall=75
2024-12-12 05:14:24 | INFO | train_inner | epoch 001:    280 / 312876 loss=11.91, nll_loss=11.342, ppl=2596.26, wps=6676.6, ups=17.19, wpb=388.5, bsz=12.7, num_updates=280, lr=3.5e-05, gnorm=2.035, loss_scale=128, train_wall=1, wall=76
2024-12-12 05:14:25 | INFO | train_inner | epoch 001:    290 / 312876 loss=12.143, nll_loss=11.599, ppl=3101.58, wps=6287.9, ups=17.79, wpb=353.5, bsz=10, num_updates=290, lr=3.625e-05, gnorm=2.057, loss_scale=128, train_wall=1, wall=76
2024-12-12 05:14:25 | INFO | train_inner | epoch 001:    300 / 312876 loss=12.073, nll_loss=11.513, ppl=2923.33, wps=6874.1, ups=18.02, wpb=381.4, bsz=12.4, num_updates=300, lr=3.75e-05, gnorm=2.246, loss_scale=128, train_wall=1, wall=77
2024-12-12 05:14:26 | INFO | train_inner | epoch 001:    310 / 312876 loss=12.151, nll_loss=11.597, ppl=3098.53, wps=6452.3, ups=17.72, wpb=364.1, bsz=13, num_updates=310, lr=3.875e-05, gnorm=2.169, loss_scale=128, train_wall=1, wall=78
2024-12-12 05:14:26 | INFO | train_inner | epoch 001:    320 / 312876 loss=11.942, nll_loss=11.365, ppl=2638.07, wps=6796.3, ups=17.26, wpb=393.8, bsz=10.9, num_updates=320, lr=4e-05, gnorm=1.92, loss_scale=128, train_wall=1, wall=78
2024-12-12 05:14:27 | INFO | train_inner | epoch 001:    330 / 312876 loss=11.609, nll_loss=10.983, ppl=2023.31, wps=6655.9, ups=17.43, wpb=381.9, bsz=13.1, num_updates=330, lr=4.125e-05, gnorm=1.944, loss_scale=128, train_wall=1, wall=79
2024-12-12 05:14:28 | INFO | train_inner | epoch 001:    340 / 312876 loss=11.695, nll_loss=11.069, ppl=2148.63, wps=6993.4, ups=18.06, wpb=387.2, bsz=16.6, num_updates=340, lr=4.25e-05, gnorm=2.129, loss_scale=128, train_wall=1, wall=79
2024-12-12 05:14:28 | INFO | train_inner | epoch 001:    350 / 312876 loss=11.718, nll_loss=11.093, ppl=2183.95, wps=6473.3, ups=17.93, wpb=361, bsz=13.5, num_updates=350, lr=4.375e-05, gnorm=2.452, loss_scale=128, train_wall=1, wall=80
2024-12-12 05:14:29 | INFO | train_inner | epoch 001:    360 / 312876 loss=11.832, nll_loss=11.218, ppl=2381.98, wps=6427.5, ups=17.95, wpb=358, bsz=11, num_updates=360, lr=4.5e-05, gnorm=2.111, loss_scale=128, train_wall=1, wall=80
2024-12-12 05:14:29 | INFO | train_inner | epoch 001:    370 / 312876 loss=11.731, nll_loss=11.11, ppl=2209.58, wps=6574.9, ups=17.81, wpb=369.2, bsz=11.5, num_updates=370, lr=4.625e-05, gnorm=2.133, loss_scale=128, train_wall=1, wall=81
2024-12-12 05:14:30 | INFO | train_inner | epoch 001:    380 / 312876 loss=11.854, nll_loss=11.245, ppl=2426.61, wps=7032.4, ups=17.62, wpb=399.2, bsz=12.8, num_updates=380, lr=4.75e-05, gnorm=1.915, loss_scale=128, train_wall=1, wall=82
2024-12-12 05:14:30 | INFO | train_inner | epoch 001:    390 / 312876 loss=11.638, nll_loss=11, ppl=2048.27, wps=6027.1, ups=17.48, wpb=344.8, bsz=9.6, num_updates=390, lr=4.875e-05, gnorm=2.016, loss_scale=128, train_wall=1, wall=82
2024-12-12 05:14:31 | INFO | train_inner | epoch 001:    400 / 312876 loss=11.601, nll_loss=10.95, ppl=1978.78, wps=6410.1, ups=17.7, wpb=362.2, bsz=11.9, num_updates=400, lr=5e-05, gnorm=2.006, loss_scale=128, train_wall=1, wall=83
2024-12-12 05:14:31 | INFO | train_inner | epoch 001:    410 / 312876 loss=12.002, nll_loss=11.402, ppl=2705.38, wps=6609.8, ups=17.6, wpb=375.5, bsz=8.8, num_updates=410, lr=5.125e-05, gnorm=2.198, loss_scale=128, train_wall=1, wall=83
2024-12-12 05:14:32 | INFO | train_inner | epoch 001:    420 / 312876 loss=11.501, nll_loss=10.848, ppl=1843.26, wps=6243.9, ups=17.62, wpb=354.4, bsz=13.6, num_updates=420, lr=5.25e-05, gnorm=2.106, loss_scale=128, train_wall=1, wall=84
2024-12-12 05:14:33 | INFO | train_inner | epoch 001:    430 / 312876 loss=11.572, nll_loss=10.915, ppl=1930.8, wps=6661.5, ups=17.78, wpb=374.7, bsz=12.5, num_updates=430, lr=5.375e-05, gnorm=1.98, loss_scale=128, train_wall=1, wall=84
2024-12-12 05:14:33 | INFO | train_inner | epoch 001:    440 / 312876 loss=11.618, nll_loss=10.963, ppl=1996.22, wps=6748.1, ups=17.31, wpb=389.8, bsz=15, num_updates=440, lr=5.5e-05, gnorm=2.084, loss_scale=128, train_wall=1, wall=85
2024-12-12 05:14:34 | INFO | train_inner | epoch 001:    450 / 312876 loss=11.515, nll_loss=10.852, ppl=1848.89, wps=6487.6, ups=17.49, wpb=371, bsz=12.5, num_updates=450, lr=5.625e-05, gnorm=1.9, loss_scale=128, train_wall=1, wall=86
2024-12-12 05:14:34 | INFO | train_inner | epoch 001:    460 / 312876 loss=11.37, nll_loss=10.683, ppl=1644.23, wps=6635, ups=17.95, wpb=369.6, bsz=15.2, num_updates=460, lr=5.75e-05, gnorm=2.068, loss_scale=128, train_wall=1, wall=86
2024-12-12 05:14:35 | INFO | train_inner | epoch 001:    470 / 312876 loss=11.661, nll_loss=11.007, ppl=2058.48, wps=6566.9, ups=17.69, wpb=371.2, bsz=16.8, num_updates=470, lr=5.875e-05, gnorm=2.859, loss_scale=128, train_wall=1, wall=87
2024-12-12 05:14:35 | INFO | train_inner | epoch 001:    480 / 312876 loss=11.656, nll_loss=11.016, ppl=2070.59, wps=6356.6, ups=17.97, wpb=353.8, bsz=10.9, num_updates=480, lr=6e-05, gnorm=2.117, loss_scale=128, train_wall=1, wall=87
2024-12-12 05:14:36 | INFO | train_inner | epoch 001:    490 / 312876 loss=11.666, nll_loss=11.015, ppl=2068.94, wps=6018, ups=17.87, wpb=336.8, bsz=9.6, num_updates=490, lr=6.125e-05, gnorm=2.125, loss_scale=128, train_wall=1, wall=88
2024-12-12 05:14:37 | INFO | train_inner | epoch 001:    500 / 312876 loss=11.67, nll_loss=11.022, ppl=2079.15, wps=6689.5, ups=17.48, wpb=382.6, bsz=15.9, num_updates=500, lr=6.25e-05, gnorm=2.242, loss_scale=128, train_wall=1, wall=88
2024-12-12 05:14:37 | INFO | train_inner | epoch 001:    510 / 312876 loss=11.421, nll_loss=10.749, ppl=1721.46, wps=6200.3, ups=17.38, wpb=356.8, bsz=17.6, num_updates=510, lr=6.375e-05, gnorm=2.269, loss_scale=128, train_wall=1, wall=89
2024-12-12 05:14:38 | INFO | train_inner | epoch 001:    520 / 312876 loss=11.635, nll_loss=10.985, ppl=2027.38, wps=6557.9, ups=17.47, wpb=375.4, bsz=13.2, num_updates=520, lr=6.5e-05, gnorm=1.944, loss_scale=128, train_wall=1, wall=89
2024-12-12 05:14:38 | INFO | train_inner | epoch 001:    530 / 312876 loss=11.463, nll_loss=10.79, ppl=1770.64, wps=6455.8, ups=17.66, wpb=365.5, bsz=11.7, num_updates=530, lr=6.625e-05, gnorm=1.989, loss_scale=128, train_wall=1, wall=90
2024-12-12 05:14:39 | INFO | train_inner | epoch 001:    540 / 312876 loss=11.539, nll_loss=10.874, ppl=1876.59, wps=6514.8, ups=17.83, wpb=365.4, bsz=10.9, num_updates=540, lr=6.75e-05, gnorm=2.028, loss_scale=128, train_wall=1, wall=91
2024-12-12 05:14:39 | INFO | train_inner | epoch 001:    550 / 312876 loss=11.471, nll_loss=10.806, ppl=1790.73, wps=7043.6, ups=17.04, wpb=413.3, bsz=11.2, num_updates=550, lr=6.875e-05, gnorm=1.931, loss_scale=128, train_wall=1, wall=91
2024-12-12 05:14:40 | INFO | train_inner | epoch 001:    560 / 312876 loss=11.554, nll_loss=10.895, ppl=1904, wps=6840.2, ups=17.67, wpb=387.2, bsz=12.8, num_updates=560, lr=7e-05, gnorm=2.03, loss_scale=128, train_wall=1, wall=92
2024-12-12 05:14:41 | INFO | train_inner | epoch 001:    570 / 312876 loss=11.485, nll_loss=10.821, ppl=1809.29, wps=6457.4, ups=17.52, wpb=368.5, bsz=11.9, num_updates=570, lr=7.125e-05, gnorm=2.056, loss_scale=128, train_wall=1, wall=92
2024-12-12 05:14:41 | INFO | train_inner | epoch 001:    580 / 312876 loss=11.282, nll_loss=10.568, ppl=1517.59, wps=6602.9, ups=17.15, wpb=384.9, bsz=18.8, num_updates=580, lr=7.25e-05, gnorm=2.444, loss_scale=128, train_wall=1, wall=93
2024-12-12 05:14:42 | INFO | train_inner | epoch 001:    590 / 312876 loss=11.47, nll_loss=10.805, ppl=1788.69, wps=6917.9, ups=17, wpb=407, bsz=15.9, num_updates=590, lr=7.375e-05, gnorm=2.083, loss_scale=128, train_wall=1, wall=94
2024-12-12 05:14:42 | INFO | train_inner | epoch 001:    600 / 312876 loss=11.303, nll_loss=10.608, ppl=1560.7, wps=6468, ups=17.36, wpb=372.5, bsz=12.3, num_updates=600, lr=7.5e-05, gnorm=2.409, loss_scale=128, train_wall=1, wall=94
2024-12-12 05:14:43 | INFO | train_inner | epoch 001:    610 / 312876 loss=11.68, nll_loss=11.036, ppl=2099.09, wps=6694.6, ups=17.37, wpb=385.3, bsz=11, num_updates=610, lr=7.625e-05, gnorm=1.924, loss_scale=128, train_wall=1, wall=95
2024-12-12 05:14:43 | INFO | train_inner | epoch 001:    620 / 312876 loss=11.63, nll_loss=10.98, ppl=2019.35, wps=6932.4, ups=17.37, wpb=399.2, bsz=11, num_updates=620, lr=7.75e-05, gnorm=1.849, loss_scale=128, train_wall=1, wall=95
2024-12-12 05:14:44 | INFO | train_inner | epoch 001:    630 / 312876 loss=11.411, nll_loss=10.736, ppl=1705.18, wps=6674.2, ups=17.2, wpb=388, bsz=14.1, num_updates=630, lr=7.875e-05, gnorm=1.857, loss_scale=128, train_wall=1, wall=96
2024-12-12 05:14:45 | INFO | train_inner | epoch 001:    640 / 312876 loss=11.343, nll_loss=10.654, ppl=1611.31, wps=6712.6, ups=17.48, wpb=384, bsz=12.6, num_updates=640, lr=8e-05, gnorm=2.008, loss_scale=128, train_wall=1, wall=96
2024-12-12 05:14:45 | INFO | train_inner | epoch 001:    650 / 312876 loss=11.475, nll_loss=10.806, ppl=1790.76, wps=6494.3, ups=17.69, wpb=367.2, bsz=10.1, num_updates=650, lr=8.125e-05, gnorm=2.042, loss_scale=128, train_wall=1, wall=97
2024-12-12 05:14:46 | INFO | train_inner | epoch 001:    660 / 312876 loss=11.403, nll_loss=10.717, ppl=1683.38, wps=6804.9, ups=17.36, wpb=392, bsz=12.8, num_updates=660, lr=8.25e-05, gnorm=2.18, loss_scale=128, train_wall=1, wall=98
2024-12-12 05:14:46 | INFO | train_inner | epoch 001:    670 / 312876 loss=11.271, nll_loss=10.57, ppl=1520.36, wps=6168.1, ups=17.66, wpb=349.2, bsz=15, num_updates=670, lr=8.375e-05, gnorm=2.307, loss_scale=128, train_wall=1, wall=98
2024-12-12 05:14:47 | INFO | train_inner | epoch 001:    680 / 312876 loss=11.393, nll_loss=10.708, ppl=1672.41, wps=6325.3, ups=17.56, wpb=360.3, bsz=10.8, num_updates=680, lr=8.5e-05, gnorm=2.007, loss_scale=128, train_wall=1, wall=99
2024-12-12 05:14:47 | INFO | train_inner | epoch 001:    690 / 312876 loss=11.389, nll_loss=10.701, ppl=1664.16, wps=6155.5, ups=17.87, wpb=344.5, bsz=12.8, num_updates=690, lr=8.625e-05, gnorm=1.971, loss_scale=128, train_wall=1, wall=99
2024-12-12 05:14:48 | INFO | train_inner | epoch 001:    700 / 312876 loss=11.325, nll_loss=10.637, ppl=1592.05, wps=6629.2, ups=17.25, wpb=384.2, bsz=15.7, num_updates=700, lr=8.75e-05, gnorm=2.134, loss_scale=128, train_wall=1, wall=100
2024-12-12 05:14:49 | INFO | train_inner | epoch 001:    710 / 312876 loss=11.449, nll_loss=10.779, ppl=1757.69, wps=6404.9, ups=17.71, wpb=361.6, bsz=14.4, num_updates=710, lr=8.875e-05, gnorm=1.993, loss_scale=128, train_wall=1, wall=100
2024-12-12 05:14:49 | INFO | train_inner | epoch 001:    720 / 312876 loss=11.297, nll_loss=10.604, ppl=1556.05, wps=6860.9, ups=17.32, wpb=396.2, bsz=14.2, num_updates=720, lr=9e-05, gnorm=1.992, loss_scale=128, train_wall=1, wall=101
2024-12-12 05:14:50 | INFO | train_inner | epoch 001:    730 / 312876 loss=11.242, nll_loss=10.541, ppl=1490.11, wps=6554.5, ups=17.7, wpb=370.4, bsz=16, num_updates=730, lr=9.125e-05, gnorm=2.267, loss_scale=128, train_wall=1, wall=102
2024-12-12 05:14:50 | INFO | train_inner | epoch 001:    740 / 312876 loss=11.455, nll_loss=10.771, ppl=1747.59, wps=6396.3, ups=17.57, wpb=364, bsz=12, num_updates=740, lr=9.25e-05, gnorm=2.281, loss_scale=128, train_wall=1, wall=102
2024-12-12 05:14:51 | INFO | train_inner | epoch 001:    750 / 312876 loss=11.305, nll_loss=10.621, ppl=1574.45, wps=6102.7, ups=17.69, wpb=345, bsz=10.8, num_updates=750, lr=9.375e-05, gnorm=2.405, loss_scale=128, train_wall=1, wall=103
2024-12-12 05:14:51 | INFO | train_inner | epoch 001:    760 / 312876 loss=11.074, nll_loss=10.341, ppl=1297.16, wps=6790.1, ups=17.13, wpb=396.4, bsz=12.5, num_updates=760, lr=9.5e-05, gnorm=1.992, loss_scale=128, train_wall=1, wall=103
2024-12-12 05:14:52 | INFO | train_inner | epoch 001:    770 / 312876 loss=11.195, nll_loss=10.482, ppl=1430.63, wps=6570.7, ups=17.33, wpb=379.1, bsz=12.7, num_updates=770, lr=9.625e-05, gnorm=2.333, loss_scale=128, train_wall=1, wall=104
2024-12-12 05:14:53 | INFO | train_inner | epoch 001:    780 / 312876 loss=11.251, nll_loss=10.559, ppl=1508.56, wps=6759.4, ups=17.42, wpb=388, bsz=13.2, num_updates=780, lr=9.75e-05, gnorm=1.966, loss_scale=128, train_wall=1, wall=104
2024-12-12 05:14:53 | INFO | train_inner | epoch 001:    790 / 312876 loss=11.172, nll_loss=10.456, ppl=1405.08, wps=6659.3, ups=17.41, wpb=382.4, bsz=16, num_updates=790, lr=9.875e-05, gnorm=1.964, loss_scale=128, train_wall=1, wall=105
2024-12-12 05:14:54 | INFO | train_inner | epoch 001:    800 / 312876 loss=11.351, nll_loss=10.672, ppl=1631.94, wps=6726.7, ups=17.19, wpb=391.4, bsz=13.4, num_updates=800, lr=0.0001, gnorm=2.083, loss_scale=128, train_wall=1, wall=106
2024-12-12 05:14:54 | INFO | train_inner | epoch 001:    810 / 312876 loss=11.478, nll_loss=10.804, ppl=1787.34, wps=6659.4, ups=17.35, wpb=383.9, bsz=10.3, num_updates=810, lr=0.00010125, gnorm=2.176, loss_scale=128, train_wall=1, wall=106
2024-12-12 05:14:55 | INFO | train_inner | epoch 001:    820 / 312876 loss=11.446, nll_loss=10.785, ppl=1764.59, wps=6315.8, ups=17.46, wpb=361.8, bsz=10.2, num_updates=820, lr=0.0001025, gnorm=1.964, loss_scale=128, train_wall=1, wall=107
2024-12-12 05:14:56 | INFO | train_inner | epoch 001:    830 / 312876 loss=11.092, nll_loss=10.367, ppl=1320.35, wps=6481.9, ups=17.39, wpb=372.8, bsz=11.9, num_updates=830, lr=0.00010375, gnorm=1.905, loss_scale=128, train_wall=1, wall=107
2024-12-12 05:14:56 | INFO | train_inner | epoch 001:    840 / 312876 loss=11.003, nll_loss=10.263, ppl=1228.62, wps=6254.3, ups=17.53, wpb=356.8, bsz=12.8, num_updates=840, lr=0.000105, gnorm=2.402, loss_scale=128, train_wall=1, wall=108
2024-12-12 05:14:57 | INFO | train_inner | epoch 001:    850 / 312876 loss=11.144, nll_loss=10.434, ppl=1383.72, wps=5842.6, ups=17.71, wpb=329.9, bsz=10.3, num_updates=850, lr=0.00010625, gnorm=2.237, loss_scale=128, train_wall=1, wall=108
2024-12-12 05:14:57 | INFO | train_inner | epoch 001:    860 / 312876 loss=11.266, nll_loss=10.571, ppl=1521.47, wps=6731.4, ups=17.22, wpb=390.8, bsz=11.9, num_updates=860, lr=0.0001075, gnorm=1.886, loss_scale=128, train_wall=1, wall=109
2024-12-12 05:14:58 | INFO | train_inner | epoch 001:    870 / 312876 loss=11.113, nll_loss=10.395, ppl=1346.45, wps=6575.6, ups=17.48, wpb=376.2, bsz=11.7, num_updates=870, lr=0.00010875, gnorm=1.979, loss_scale=128, train_wall=1, wall=110
2024-12-12 05:14:58 | INFO | train_inner | epoch 001:    880 / 312876 loss=11.298, nll_loss=10.61, ppl=1562.89, wps=6527.2, ups=17.94, wpb=363.8, bsz=10.4, num_updates=880, lr=0.00011, gnorm=1.943, loss_scale=128, train_wall=1, wall=110
2024-12-12 05:14:59 | INFO | train_inner | epoch 001:    890 / 312876 loss=11.068, nll_loss=10.347, ppl=1302.64, wps=7281, ups=17.43, wpb=417.8, bsz=18.2, num_updates=890, lr=0.00011125, gnorm=2.069, loss_scale=128, train_wall=1, wall=111
2024-12-12 05:15:00 | INFO | train_inner | epoch 001:    900 / 312876 loss=11.151, nll_loss=10.428, ppl=1377.68, wps=6951.3, ups=17.41, wpb=399.2, bsz=13.2, num_updates=900, lr=0.0001125, gnorm=2.032, loss_scale=128, train_wall=1, wall=111
2024-12-12 05:15:00 | INFO | train_inner | epoch 001:    910 / 312876 loss=11.199, nll_loss=10.504, ppl=1451.76, wps=6597, ups=17.56, wpb=375.6, bsz=9.3, num_updates=910, lr=0.00011375, gnorm=1.971, loss_scale=128, train_wall=1, wall=112
2024-12-12 05:15:01 | INFO | train_inner | epoch 001:    920 / 312876 loss=11.277, nll_loss=10.583, ppl=1533.81, wps=6940.3, ups=17.42, wpb=398.3, bsz=10.3, num_updates=920, lr=0.000115, gnorm=1.879, loss_scale=128, train_wall=1, wall=112
2024-12-12 05:15:01 | INFO | train_inner | epoch 001:    930 / 312876 loss=11.268, nll_loss=10.576, ppl=1526.84, wps=6277.1, ups=17.02, wpb=368.9, bsz=10.7, num_updates=930, lr=0.00011625, gnorm=2.153, loss_scale=128, train_wall=1, wall=113
2024-12-12 05:15:02 | INFO | train_inner | epoch 001:    940 / 312876 loss=11.054, nll_loss=10.325, ppl=1282.69, wps=6566.7, ups=17.28, wpb=380, bsz=14.4, num_updates=940, lr=0.0001175, gnorm=2.308, loss_scale=128, train_wall=1, wall=114
2024-12-12 05:15:02 | INFO | train_inner | epoch 001:    950 / 312876 loss=11.164, nll_loss=10.436, ppl=1385.13, wps=6365.6, ups=16.89, wpb=376.8, bsz=19, num_updates=950, lr=0.00011875, gnorm=2.192, loss_scale=128, train_wall=1, wall=114
2024-12-12 05:15:03 | INFO | train_inner | epoch 001:    960 / 312876 loss=11.005, nll_loss=10.274, ppl=1238.23, wps=6745.7, ups=16.9, wpb=399.2, bsz=12, num_updates=960, lr=0.00012, gnorm=1.919, loss_scale=128, train_wall=1, wall=115
2024-12-12 05:15:04 | INFO | train_inner | epoch 001:    970 / 312876 loss=11.15, nll_loss=10.446, ppl=1394.89, wps=6756.7, ups=17.45, wpb=387.1, bsz=10.3, num_updates=970, lr=0.00012125, gnorm=1.965, loss_scale=128, train_wall=1, wall=115
2024-12-12 05:15:04 | INFO | train_inner | epoch 001:    980 / 312876 loss=10.877, nll_loss=10.126, ppl=1117.67, wps=6744.7, ups=16.94, wpb=398.1, bsz=16.7, num_updates=980, lr=0.0001225, gnorm=2.163, loss_scale=128, train_wall=1, wall=116
2024-12-12 05:15:05 | INFO | train_inner | epoch 001:    990 / 312876 loss=10.907, nll_loss=10.169, ppl=1151.09, wps=6143.3, ups=17.29, wpb=355.3, bsz=11.1, num_updates=990, lr=0.00012375, gnorm=1.979, loss_scale=128, train_wall=1, wall=116
2024-12-12 05:15:05 | INFO | train_inner | epoch 001:   1000 / 312876 loss=10.82, nll_loss=10.079, ppl=1081.65, wps=5583.4, ups=17.96, wpb=310.8, bsz=14.2, num_updates=1000, lr=0.000125, gnorm=2.265, loss_scale=128, train_wall=1, wall=117
2024-12-12 05:15:06 | INFO | train_inner | epoch 001:   1010 / 312876 loss=10.91, nll_loss=10.166, ppl=1148.98, wps=6773.1, ups=17.17, wpb=394.4, bsz=16, num_updates=1010, lr=0.00012625, gnorm=2.019, loss_scale=128, train_wall=1, wall=118
2024-12-12 05:15:06 | INFO | train_inner | epoch 001:   1020 / 312876 loss=11.061, nll_loss=10.346, ppl=1301.8, wps=6425.7, ups=17.45, wpb=368.3, bsz=10.2, num_updates=1020, lr=0.0001275, gnorm=2.079, loss_scale=128, train_wall=1, wall=118
2024-12-12 05:15:07 | INFO | train_inner | epoch 001:   1030 / 312876 loss=10.892, nll_loss=10.138, ppl=1126.88, wps=6749.7, ups=17.4, wpb=388, bsz=12.8, num_updates=1030, lr=0.00012875, gnorm=1.882, loss_scale=128, train_wall=1, wall=119
2024-12-12 05:15:08 | INFO | train_inner | epoch 001:   1040 / 312876 loss=10.99, nll_loss=10.273, ppl=1237.31, wps=6071.3, ups=17.45, wpb=348, bsz=14.4, num_updates=1040, lr=0.00013, gnorm=2.066, loss_scale=128, train_wall=1, wall=119
2024-12-12 05:15:08 | INFO | train_inner | epoch 001:   1050 / 312876 loss=11.125, nll_loss=10.412, ppl=1362.2, wps=6113.4, ups=17.52, wpb=349, bsz=10.6, num_updates=1050, lr=0.00013125, gnorm=2.21, loss_scale=128, train_wall=1, wall=120
2024-12-12 05:15:09 | INFO | train_inner | epoch 001:   1060 / 312876 loss=10.954, nll_loss=10.219, ppl=1192.12, wps=6696.7, ups=17.37, wpb=385.5, bsz=11.1, num_updates=1060, lr=0.0001325, gnorm=1.96, loss_scale=128, train_wall=1, wall=121
2024-12-12 05:15:09 | INFO | train_inner | epoch 001:   1070 / 312876 loss=11.058, nll_loss=10.338, ppl=1294.25, wps=6321.8, ups=17.37, wpb=364, bsz=11.2, num_updates=1070, lr=0.00013375, gnorm=1.984, loss_scale=128, train_wall=1, wall=121
2024-12-12 05:15:10 | INFO | train_inner | epoch 001:   1080 / 312876 loss=11.08, nll_loss=10.369, ppl=1322.76, wps=6881.7, ups=17.17, wpb=400.8, bsz=13.6, num_updates=1080, lr=0.000135, gnorm=1.88, loss_scale=128, train_wall=1, wall=122
2024-12-12 05:15:11 | INFO | train_inner | epoch 001:   1090 / 312876 loss=11.185, nll_loss=10.496, ppl=1444.11, wps=6829.5, ups=17.25, wpb=395.9, bsz=13.8, num_updates=1090, lr=0.00013625, gnorm=2.028, loss_scale=128, train_wall=1, wall=122
2024-12-12 05:15:11 | INFO | train_inner | epoch 001:   1100 / 312876 loss=10.804, nll_loss=10.035, ppl=1049.05, wps=6803.5, ups=17.22, wpb=395, bsz=17.5, num_updates=1100, lr=0.0001375, gnorm=2.217, loss_scale=128, train_wall=1, wall=123
2024-12-12 05:15:12 | INFO | train_inner | epoch 001:   1110 / 312876 loss=11.35, nll_loss=10.672, ppl=1631.81, wps=5996.3, ups=17.67, wpb=339.3, bsz=9.8, num_updates=1110, lr=0.00013875, gnorm=2.427, loss_scale=128, train_wall=1, wall=123
2024-12-12 05:15:12 | INFO | train_inner | epoch 001:   1120 / 312876 loss=10.84, nll_loss=10.103, ppl=1099.51, wps=6369.4, ups=17.58, wpb=362.4, bsz=12, num_updates=1120, lr=0.00014, gnorm=2.146, loss_scale=128, train_wall=1, wall=124
2024-12-12 05:15:13 | INFO | train_inner | epoch 001:   1130 / 312876 loss=10.917, nll_loss=10.153, ppl=1138.9, wps=6594.2, ups=17.42, wpb=378.6, bsz=12.8, num_updates=1130, lr=0.00014125, gnorm=2.016, loss_scale=128, train_wall=1, wall=125
2024-12-12 05:15:13 | INFO | train_inner | epoch 001:   1140 / 312876 loss=11.191, nll_loss=10.498, ppl=1446.55, wps=6896.4, ups=17.51, wpb=393.9, bsz=9.5, num_updates=1140, lr=0.0001425, gnorm=2.091, loss_scale=128, train_wall=1, wall=125
2024-12-12 05:15:14 | INFO | train_inner | epoch 001:   1150 / 312876 loss=10.846, nll_loss=10.095, ppl=1093.97, wps=6710.6, ups=17.55, wpb=382.4, bsz=16, num_updates=1150, lr=0.00014375, gnorm=2.264, loss_scale=128, train_wall=1, wall=126
2024-12-12 05:15:15 | INFO | train_inner | epoch 001:   1160 / 312876 loss=10.848, nll_loss=10.092, ppl=1091.67, wps=7240.3, ups=17.17, wpb=421.7, bsz=16.6, num_updates=1160, lr=0.000145, gnorm=1.824, loss_scale=128, train_wall=1, wall=126
2024-12-12 05:15:15 | INFO | train_inner | epoch 001:   1170 / 312876 loss=10.871, nll_loss=10.137, ppl=1125.97, wps=6672.6, ups=17.28, wpb=386.1, bsz=12.5, num_updates=1170, lr=0.00014625, gnorm=1.957, loss_scale=128, train_wall=1, wall=127
2024-12-12 05:15:16 | INFO | train_inner | epoch 001:   1180 / 312876 loss=11.165, nll_loss=10.461, ppl=1409.16, wps=6353.5, ups=17.69, wpb=359.1, bsz=7.2, num_updates=1180, lr=0.0001475, gnorm=1.993, loss_scale=128, train_wall=1, wall=127
2024-12-12 05:15:16 | INFO | train_inner | epoch 001:   1190 / 312876 loss=11.085, nll_loss=10.381, ppl=1333.49, wps=6057.5, ups=17.73, wpb=341.6, bsz=10.4, num_updates=1190, lr=0.00014875, gnorm=2.109, loss_scale=128, train_wall=1, wall=128
2024-12-12 05:15:17 | INFO | train_inner | epoch 001:   1200 / 312876 loss=11.136, nll_loss=10.429, ppl=1378.19, wps=6727.4, ups=17.37, wpb=387.2, bsz=13.6, num_updates=1200, lr=0.00015, gnorm=1.832, loss_scale=128, train_wall=1, wall=129
2024-12-12 05:15:17 | INFO | train_inner | epoch 001:   1210 / 312876 loss=10.984, nll_loss=10.262, ppl=1227.99, wps=6556.4, ups=17.36, wpb=377.6, bsz=15.2, num_updates=1210, lr=0.00015125, gnorm=2.139, loss_scale=128, train_wall=1, wall=129
2024-12-12 05:15:18 | INFO | train_inner | epoch 001:   1220 / 312876 loss=10.863, nll_loss=10.115, ppl=1109.29, wps=5835.9, ups=17.62, wpb=331.2, bsz=10.4, num_updates=1220, lr=0.0001525, gnorm=2.024, loss_scale=128, train_wall=1, wall=130
2024-12-12 05:15:19 | INFO | train_inner | epoch 001:   1230 / 312876 loss=10.909, nll_loss=10.169, ppl=1151.15, wps=6950.1, ups=17.17, wpb=404.8, bsz=14.4, num_updates=1230, lr=0.00015375, gnorm=1.824, loss_scale=128, train_wall=1, wall=130
2024-12-12 05:15:19 | INFO | train_inner | epoch 001:   1240 / 312876 loss=10.891, nll_loss=10.152, ppl=1137.54, wps=6332, ups=17.63, wpb=359.2, bsz=12, num_updates=1240, lr=0.000155, gnorm=2.104, loss_scale=128, train_wall=1, wall=131
2024-12-12 05:15:20 | INFO | train_inner | epoch 001:   1250 / 312876 loss=10.835, nll_loss=10.091, ppl=1090.5, wps=6584.9, ups=17.78, wpb=370.4, bsz=14.4, num_updates=1250, lr=0.00015625, gnorm=2.223, loss_scale=128, train_wall=1, wall=131
2024-12-12 05:15:20 | INFO | train_inner | epoch 001:   1260 / 312876 loss=11.07, nll_loss=10.352, ppl=1306.76, wps=5484, ups=17.13, wpb=320.2, bsz=8.6, num_updates=1260, lr=0.0001575, gnorm=2.171, loss_scale=128, train_wall=1, wall=132
2024-12-12 05:15:21 | INFO | train_inner | epoch 001:   1270 / 312876 loss=10.657, nll_loss=9.9, ppl=955.49, wps=6617.2, ups=17.31, wpb=382.2, bsz=18.7, num_updates=1270, lr=0.00015875, gnorm=2.128, loss_scale=128, train_wall=1, wall=133
2024-12-12 05:15:21 | INFO | train_inner | epoch 001:   1280 / 312876 loss=10.955, nll_loss=10.21, ppl=1184.07, wps=6109.9, ups=17.16, wpb=356, bsz=13.6, num_updates=1280, lr=0.00016, gnorm=1.852, loss_scale=128, train_wall=1, wall=133
2024-12-12 05:15:22 | INFO | train_inner | epoch 001:   1290 / 312876 loss=10.98, nll_loss=10.26, ppl=1225.83, wps=6604.8, ups=17.34, wpb=381, bsz=11.9, num_updates=1290, lr=0.00016125, gnorm=1.943, loss_scale=128, train_wall=1, wall=134
2024-12-12 05:15:23 | INFO | train_inner | epoch 001:   1300 / 312876 loss=10.843, nll_loss=10.099, ppl=1096.95, wps=6345.1, ups=17.47, wpb=363.2, bsz=12.4, num_updates=1300, lr=0.0001625, gnorm=2.141, loss_scale=128, train_wall=1, wall=134
2024-12-12 05:15:23 | INFO | train_inner | epoch 001:   1310 / 312876 loss=10.827, nll_loss=10.08, ppl=1082.35, wps=7150.7, ups=17.23, wpb=415.1, bsz=15.1, num_updates=1310, lr=0.00016375, gnorm=1.946, loss_scale=128, train_wall=1, wall=135
2024-12-12 05:15:24 | INFO | train_inner | epoch 001:   1320 / 312876 loss=11.025, nll_loss=10.306, ppl=1266.23, wps=6507.8, ups=17.25, wpb=377.2, bsz=14, num_updates=1320, lr=0.000165, gnorm=2.012, loss_scale=128, train_wall=1, wall=135
2024-12-12 05:15:24 | INFO | train_inner | epoch 001:   1330 / 312876 loss=10.858, nll_loss=10.119, ppl=1112.18, wps=6382.7, ups=17.66, wpb=361.4, bsz=13.2, num_updates=1330, lr=0.00016625, gnorm=2.167, loss_scale=128, train_wall=1, wall=136
2024-12-12 05:15:25 | INFO | train_inner | epoch 001:   1340 / 312876 loss=10.739, nll_loss=9.976, ppl=1007.18, wps=6518.6, ups=17.41, wpb=374.4, bsz=15.2, num_updates=1340, lr=0.0001675, gnorm=2.071, loss_scale=128, train_wall=1, wall=137
2024-12-12 05:15:25 | INFO | train_inner | epoch 001:   1350 / 312876 loss=10.9, nll_loss=10.161, ppl=1145.16, wps=7048.8, ups=17.71, wpb=397.9, bsz=8.8, num_updates=1350, lr=0.00016875, gnorm=1.811, loss_scale=128, train_wall=1, wall=137
2024-12-12 05:15:26 | INFO | train_inner | epoch 001:   1360 / 312876 loss=10.609, nll_loss=9.834, ppl=912.98, wps=6570.3, ups=18.01, wpb=364.8, bsz=18.4, num_updates=1360, lr=0.00017, gnorm=2.292, loss_scale=128, train_wall=1, wall=138
2024-12-12 05:15:27 | INFO | train_inner | epoch 001:   1370 / 312876 loss=11.127, nll_loss=10.421, ppl=1370.9, wps=6351.6, ups=18, wpb=352.8, bsz=12, num_updates=1370, lr=0.00017125, gnorm=1.895, loss_scale=128, train_wall=1, wall=138
2024-12-12 05:15:27 | INFO | train_inner | epoch 001:   1380 / 312876 loss=11.109, nll_loss=10.382, ppl=1334.14, wps=7116.5, ups=17.4, wpb=409, bsz=9.5, num_updates=1380, lr=0.0001725, gnorm=1.931, loss_scale=128, train_wall=1, wall=139
2024-12-12 05:15:28 | INFO | train_inner | epoch 001:   1390 / 312876 loss=10.831, nll_loss=10.092, ppl=1091.2, wps=6879.9, ups=17.8, wpb=386.4, bsz=9.3, num_updates=1390, lr=0.00017375, gnorm=1.946, loss_scale=128, train_wall=1, wall=139
2024-12-12 05:15:28 | INFO | train_inner | epoch 001:   1400 / 312876 loss=10.834, nll_loss=10.076, ppl=1079.37, wps=6084.6, ups=17.78, wpb=342.3, bsz=10.9, num_updates=1400, lr=0.000175, gnorm=1.936, loss_scale=128, train_wall=1, wall=140
2024-12-12 05:15:29 | INFO | train_inner | epoch 001:   1410 / 312876 loss=10.958, nll_loss=10.231, ppl=1201.5, wps=7075.7, ups=17.51, wpb=404, bsz=13.6, num_updates=1410, lr=0.00017625, gnorm=1.991, loss_scale=128, train_wall=1, wall=141
2024-12-12 05:15:29 | INFO | train_inner | epoch 001:   1420 / 312876 loss=10.944, nll_loss=10.216, ppl=1189.02, wps=6713.4, ups=17.54, wpb=382.7, bsz=10, num_updates=1420, lr=0.0001775, gnorm=1.964, loss_scale=128, train_wall=1, wall=141
2024-12-12 05:15:30 | INFO | train_inner | epoch 001:   1430 / 312876 loss=11.284, nll_loss=10.6, ppl=1552.1, wps=6863.1, ups=17.67, wpb=388.4, bsz=11, num_updates=1430, lr=0.00017875, gnorm=1.858, loss_scale=128, train_wall=1, wall=142
2024-12-12 05:15:31 | INFO | train_inner | epoch 001:   1440 / 312876 loss=11.013, nll_loss=10.299, ppl=1259.84, wps=6779.1, ups=16.78, wpb=404.1, bsz=13.8, num_updates=1440, lr=0.00018, gnorm=2.034, loss_scale=128, train_wall=1, wall=142
2024-12-12 05:15:31 | INFO | train_inner | epoch 001:   1450 / 312876 loss=11.015, nll_loss=10.285, ppl=1247.72, wps=6690.7, ups=17.38, wpb=385, bsz=11.7, num_updates=1450, lr=0.00018125, gnorm=1.864, loss_scale=128, train_wall=1, wall=143
2024-12-12 05:15:32 | INFO | train_inner | epoch 001:   1460 / 312876 loss=10.765, nll_loss=10.011, ppl=1031.72, wps=6694.6, ups=17.8, wpb=376, bsz=16, num_updates=1460, lr=0.0001825, gnorm=1.972, loss_scale=128, train_wall=1, wall=143
2024-12-12 05:15:32 | INFO | train_inner | epoch 001:   1470 / 312876 loss=10.796, nll_loss=10.047, ppl=1057.95, wps=6477, ups=17.68, wpb=366.4, bsz=12.8, num_updates=1470, lr=0.00018375, gnorm=1.956, loss_scale=128, train_wall=1, wall=144
2024-12-12 05:15:33 | INFO | train_inner | epoch 001:   1480 / 312876 loss=10.782, nll_loss=10.025, ppl=1041.73, wps=6378, ups=17.28, wpb=369, bsz=13.3, num_updates=1480, lr=0.000185, gnorm=1.974, loss_scale=128, train_wall=1, wall=145
2024-12-12 05:15:33 | INFO | train_inner | epoch 001:   1490 / 312876 loss=10.975, nll_loss=10.25, ppl=1218.04, wps=6584.8, ups=17.89, wpb=368, bsz=13.6, num_updates=1490, lr=0.00018625, gnorm=2.001, loss_scale=128, train_wall=1, wall=145
2024-12-12 05:15:34 | INFO | train_inner | epoch 001:   1500 / 312876 loss=11.037, nll_loss=10.311, ppl=1269.93, wps=6858.6, ups=17.35, wpb=395.4, bsz=11.6, num_updates=1500, lr=0.0001875, gnorm=2.134, loss_scale=128, train_wall=1, wall=146
2024-12-12 05:15:35 | INFO | train_inner | epoch 001:   1510 / 312876 loss=10.901, nll_loss=10.161, ppl=1144.93, wps=6130, ups=17.63, wpb=347.6, bsz=12.6, num_updates=1510, lr=0.00018875, gnorm=2.058, loss_scale=128, train_wall=1, wall=146
2024-12-12 05:15:35 | INFO | train_inner | epoch 001:   1520 / 312876 loss=10.864, nll_loss=10.126, ppl=1117.8, wps=6747.4, ups=17.61, wpb=383.2, bsz=11.8, num_updates=1520, lr=0.00019, gnorm=2.082, loss_scale=128, train_wall=1, wall=147
2024-12-12 05:15:36 | INFO | train_inner | epoch 001:   1530 / 312876 loss=10.834, nll_loss=10.081, ppl=1083.16, wps=6903.3, ups=17.34, wpb=398.2, bsz=13.1, num_updates=1530, lr=0.00019125, gnorm=1.868, loss_scale=128, train_wall=1, wall=147
2024-12-12 05:15:36 | INFO | train_inner | epoch 001:   1540 / 312876 loss=10.692, nll_loss=9.923, ppl=971.05, wps=7174.5, ups=17.37, wpb=413.1, bsz=10, num_updates=1540, lr=0.0001925, gnorm=1.887, loss_scale=128, train_wall=1, wall=148
2024-12-12 05:15:37 | INFO | train_inner | epoch 001:   1550 / 312876 loss=10.799, nll_loss=10.053, ppl=1062.32, wps=6451.3, ups=17.38, wpb=371.2, bsz=11.2, num_updates=1550, lr=0.00019375, gnorm=1.931, loss_scale=128, train_wall=1, wall=149
2024-12-12 05:15:37 | INFO | train_inner | epoch 001:   1560 / 312876 loss=10.406, nll_loss=9.599, ppl=775.24, wps=6761.8, ups=16.96, wpb=398.8, bsz=18, num_updates=1560, lr=0.000195, gnorm=2.128, loss_scale=128, train_wall=1, wall=149
2024-12-12 05:15:38 | INFO | train_inner | epoch 001:   1570 / 312876 loss=10.904, nll_loss=10.167, ppl=1149.56, wps=6955.6, ups=17.14, wpb=405.8, bsz=12.4, num_updates=1570, lr=0.00019625, gnorm=1.807, loss_scale=128, train_wall=1, wall=150
2024-12-12 05:15:39 | INFO | train_inner | epoch 001:   1580 / 312876 loss=10.697, nll_loss=9.929, ppl=974.72, wps=6599, ups=17.6, wpb=375, bsz=12.4, num_updates=1580, lr=0.0001975, gnorm=1.957, loss_scale=128, train_wall=1, wall=150
2024-12-12 05:15:39 | INFO | train_inner | epoch 001:   1590 / 312876 loss=10.582, nll_loss=9.795, ppl=888.32, wps=6769.7, ups=17.42, wpb=388.7, bsz=13.2, num_updates=1590, lr=0.00019875, gnorm=1.865, loss_scale=128, train_wall=1, wall=151
2024-12-12 05:15:40 | INFO | train_inner | epoch 001:   1600 / 312876 loss=10.823, nll_loss=10.086, ppl=1086.99, wps=6932.9, ups=17.47, wpb=396.8, bsz=14.4, num_updates=1600, lr=0.0002, gnorm=1.861, loss_scale=128, train_wall=1, wall=151
2024-12-12 05:15:40 | INFO | train_inner | epoch 001:   1610 / 312876 loss=10.659, nll_loss=9.88, ppl=942.34, wps=6915.6, ups=17.28, wpb=400.3, bsz=16, num_updates=1610, lr=0.00020125, gnorm=2.071, loss_scale=128, train_wall=1, wall=152
2024-12-12 05:15:41 | INFO | train_inner | epoch 001:   1620 / 312876 loss=10.608, nll_loss=9.836, ppl=913.78, wps=6302.4, ups=17.56, wpb=359, bsz=14.2, num_updates=1620, lr=0.0002025, gnorm=2.042, loss_scale=128, train_wall=1, wall=153
2024-12-12 05:15:41 | INFO | train_inner | epoch 001:   1630 / 312876 loss=10.811, nll_loss=10.066, ppl=1071.93, wps=6868.3, ups=17.27, wpb=397.7, bsz=15.2, num_updates=1630, lr=0.00020375, gnorm=1.844, loss_scale=128, train_wall=1, wall=153
2024-12-12 05:15:42 | INFO | train_inner | epoch 001:   1640 / 312876 loss=10.931, nll_loss=10.2, ppl=1176.56, wps=6646.7, ups=17.32, wpb=383.7, bsz=7.8, num_updates=1640, lr=0.000205, gnorm=1.979, loss_scale=128, train_wall=1, wall=154
2024-12-12 05:15:43 | INFO | train_inner | epoch 001:   1650 / 312876 loss=10.544, nll_loss=9.75, ppl=861.06, wps=6659.4, ups=17.5, wpb=380.6, bsz=14.3, num_updates=1650, lr=0.00020625, gnorm=1.869, loss_scale=128, train_wall=1, wall=154
2024-12-12 05:15:43 | INFO | train_inner | epoch 001:   1660 / 312876 loss=10.447, nll_loss=9.649, ppl=802.83, wps=7518.5, ups=17.09, wpb=440, bsz=16.8, num_updates=1660, lr=0.0002075, gnorm=1.905, loss_scale=128, train_wall=1, wall=155
2024-12-12 05:15:44 | INFO | train_inner | epoch 001:   1670 / 312876 loss=10.866, nll_loss=10.131, ppl=1121.69, wps=6955.9, ups=17.61, wpb=395, bsz=12.5, num_updates=1670, lr=0.00020875, gnorm=2.06, loss_scale=128, train_wall=1, wall=155
2024-12-12 05:15:44 | INFO | train_inner | epoch 001:   1680 / 312876 loss=10.547, nll_loss=9.756, ppl=864.8, wps=6313.4, ups=17.77, wpb=355.2, bsz=13.5, num_updates=1680, lr=0.00021, gnorm=2.078, loss_scale=128, train_wall=1, wall=156
2024-12-12 05:15:45 | INFO | train_inner | epoch 001:   1690 / 312876 loss=10.788, nll_loss=10.04, ppl=1052.42, wps=6927.6, ups=17.56, wpb=394.6, bsz=12.4, num_updates=1690, lr=0.00021125, gnorm=1.922, loss_scale=128, train_wall=1, wall=157
2024-12-12 05:15:45 | INFO | train_inner | epoch 001:   1700 / 312876 loss=10.65, nll_loss=9.873, ppl=938.03, wps=6215.1, ups=17.8, wpb=349.1, bsz=10.2, num_updates=1700, lr=0.0002125, gnorm=2.035, loss_scale=128, train_wall=1, wall=157
2024-12-12 05:15:46 | INFO | train_inner | epoch 001:   1710 / 312876 loss=10.786, nll_loss=10.03, ppl=1045.42, wps=6403.1, ups=17.62, wpb=363.5, bsz=9.1, num_updates=1710, lr=0.00021375, gnorm=1.989, loss_scale=128, train_wall=1, wall=158
2024-12-12 05:15:47 | INFO | train_inner | epoch 001:   1720 / 312876 loss=10.857, nll_loss=10.108, ppl=1103.57, wps=6655.7, ups=17.42, wpb=382, bsz=13.4, num_updates=1720, lr=0.000215, gnorm=1.902, loss_scale=128, train_wall=1, wall=158
2024-12-12 05:15:47 | INFO | train_inner | epoch 001:   1730 / 312876 loss=10.742, nll_loss=9.992, ppl=1018.14, wps=7164.2, ups=17.13, wpb=418.2, bsz=11.7, num_updates=1730, lr=0.00021625, gnorm=1.895, loss_scale=128, train_wall=1, wall=159
2024-12-12 05:15:48 | INFO | train_inner | epoch 001:   1740 / 312876 loss=10.823, nll_loss=10.074, ppl=1077.86, wps=6516.6, ups=17.15, wpb=380, bsz=10.6, num_updates=1740, lr=0.0002175, gnorm=2.158, loss_scale=128, train_wall=1, wall=159
2024-12-12 05:15:48 | INFO | train_inner | epoch 001:   1750 / 312876 loss=10.644, nll_loss=9.866, ppl=932.87, wps=6953.6, ups=17.29, wpb=402.2, bsz=11.6, num_updates=1750, lr=0.00021875, gnorm=2.019, loss_scale=128, train_wall=1, wall=160
2024-12-12 05:15:49 | INFO | train_inner | epoch 001:   1760 / 312876 loss=10.925, nll_loss=10.199, ppl=1175.28, wps=6957.1, ups=17.41, wpb=399.7, bsz=13.1, num_updates=1760, lr=0.00022, gnorm=1.88, loss_scale=128, train_wall=1, wall=161
2024-12-12 05:15:49 | INFO | train_inner | epoch 001:   1770 / 312876 loss=10.763, nll_loss=10.005, ppl=1027.84, wps=6704.2, ups=17.42, wpb=384.8, bsz=12, num_updates=1770, lr=0.00022125, gnorm=2.028, loss_scale=128, train_wall=1, wall=161
2024-12-12 05:15:50 | INFO | train_inner | epoch 001:   1780 / 312876 loss=10.819, nll_loss=10.073, ppl=1077.34, wps=6928.4, ups=17.41, wpb=397.9, bsz=12.7, num_updates=1780, lr=0.0002225, gnorm=1.935, loss_scale=128, train_wall=1, wall=162
2024-12-12 05:15:51 | INFO | train_inner | epoch 001:   1790 / 312876 loss=10.797, nll_loss=10.043, ppl=1054.74, wps=6902.8, ups=17.6, wpb=392.3, bsz=11, num_updates=1790, lr=0.00022375, gnorm=1.952, loss_scale=128, train_wall=1, wall=162
2024-12-12 05:15:51 | INFO | train_inner | epoch 001:   1800 / 312876 loss=10.78, nll_loss=10.017, ppl=1035.8, wps=7338.3, ups=16.87, wpb=434.9, bsz=12, num_updates=1800, lr=0.000225, gnorm=1.92, loss_scale=128, train_wall=1, wall=163
2024-12-12 05:15:52 | INFO | train_inner | epoch 001:   1810 / 312876 loss=10.731, nll_loss=9.972, ppl=1004.62, wps=6172.5, ups=17.9, wpb=344.8, bsz=12, num_updates=1810, lr=0.00022625, gnorm=1.983, loss_scale=128, train_wall=1, wall=164
2024-12-12 05:15:52 | INFO | train_inner | epoch 001:   1820 / 312876 loss=10.686, nll_loss=9.915, ppl=965.47, wps=6673.9, ups=17.64, wpb=378.3, bsz=13.5, num_updates=1820, lr=0.0002275, gnorm=1.898, loss_scale=128, train_wall=1, wall=164
2024-12-12 05:15:53 | INFO | train_inner | epoch 001:   1830 / 312876 loss=10.772, nll_loss=10.018, ppl=1036.59, wps=6773.2, ups=17.48, wpb=387.4, bsz=11.5, num_updates=1830, lr=0.00022875, gnorm=1.906, loss_scale=128, train_wall=1, wall=165
2024-12-12 05:15:53 | INFO | train_inner | epoch 001:   1840 / 312876 loss=10.595, nll_loss=9.823, ppl=905.8, wps=6895.7, ups=17.63, wpb=391.2, bsz=14.2, num_updates=1840, lr=0.00023, gnorm=2.077, loss_scale=128, train_wall=1, wall=165
2024-12-12 05:15:54 | INFO | train_inner | epoch 001:   1850 / 312876 loss=10.898, nll_loss=10.161, ppl=1145.17, wps=6394.5, ups=17.74, wpb=360.4, bsz=9.4, num_updates=1850, lr=0.00023125, gnorm=1.979, loss_scale=128, train_wall=1, wall=166
2024-12-12 05:15:55 | INFO | train_inner | epoch 001:   1860 / 312876 loss=10.839, nll_loss=10.092, ppl=1091.79, wps=7234.9, ups=17.51, wpb=413.1, bsz=9.8, num_updates=1860, lr=0.0002325, gnorm=1.86, loss_scale=128, train_wall=1, wall=166
2024-12-12 05:15:55 | INFO | train_inner | epoch 001:   1870 / 312876 loss=10.775, nll_loss=10.015, ppl=1034.67, wps=6264.7, ups=17.53, wpb=357.3, bsz=9.6, num_updates=1870, lr=0.00023375, gnorm=1.998, loss_scale=128, train_wall=1, wall=167
2024-12-12 05:15:56 | INFO | train_inner | epoch 001:   1880 / 312876 loss=10.523, nll_loss=9.731, ppl=849.94, wps=6122.4, ups=17.88, wpb=342.4, bsz=12.8, num_updates=1880, lr=0.000235, gnorm=2.015, loss_scale=128, train_wall=1, wall=167
2024-12-12 05:15:56 | INFO | train_inner | epoch 001:   1890 / 312876 loss=10.814, nll_loss=10.059, ppl=1066.86, wps=6745.4, ups=17.75, wpb=380, bsz=9.4, num_updates=1890, lr=0.00023625, gnorm=2.082, loss_scale=128, train_wall=1, wall=168
2024-12-12 05:15:57 | INFO | train_inner | epoch 001:   1900 / 312876 loss=10.866, nll_loss=10.127, ppl=1118.28, wps=6565.9, ups=17.43, wpb=376.8, bsz=13.6, num_updates=1900, lr=0.0002375, gnorm=1.984, loss_scale=128, train_wall=1, wall=169
2024-12-12 05:15:57 | INFO | train_inner | epoch 001:   1910 / 312876 loss=10.915, nll_loss=10.178, ppl=1158.81, wps=6871.1, ups=17.26, wpb=398, bsz=10.7, num_updates=1910, lr=0.00023875, gnorm=2.021, loss_scale=128, train_wall=1, wall=169
2024-12-12 05:15:58 | INFO | train_inner | epoch 001:   1920 / 312876 loss=10.699, nll_loss=9.939, ppl=981.48, wps=6050.4, ups=17.24, wpb=350.9, bsz=13.6, num_updates=1920, lr=0.00024, gnorm=2.001, loss_scale=128, train_wall=1, wall=170
2024-12-12 05:15:59 | INFO | train_inner | epoch 001:   1930 / 312876 loss=10.412, nll_loss=9.607, ppl=779.91, wps=6458.1, ups=17.66, wpb=365.6, bsz=12.8, num_updates=1930, lr=0.00024125, gnorm=2.046, loss_scale=128, train_wall=1, wall=170
2024-12-12 05:15:59 | INFO | train_inner | epoch 001:   1940 / 312876 loss=10.668, nll_loss=9.893, ppl=950.64, wps=6536.9, ups=17.17, wpb=380.8, bsz=12, num_updates=1940, lr=0.0002425, gnorm=1.804, loss_scale=128, train_wall=1, wall=171
2024-12-12 05:16:00 | INFO | train_inner | epoch 001:   1950 / 312876 loss=10.589, nll_loss=9.811, ppl=898.48, wps=6735.5, ups=17.57, wpb=383.3, bsz=13.1, num_updates=1950, lr=0.00024375, gnorm=1.943, loss_scale=128, train_wall=1, wall=172
2024-12-12 05:16:00 | INFO | train_inner | epoch 001:   1960 / 312876 loss=10.513, nll_loss=9.72, ppl=843.23, wps=6979.7, ups=17.37, wpb=401.9, bsz=12.3, num_updates=1960, lr=0.000245, gnorm=1.894, loss_scale=128, train_wall=1, wall=172
2024-12-12 05:16:01 | INFO | train_inner | epoch 001:   1970 / 312876 loss=10.583, nll_loss=9.8, ppl=891.2, wps=6949.1, ups=17.51, wpb=396.8, bsz=9.8, num_updates=1970, lr=0.00024625, gnorm=1.862, loss_scale=128, train_wall=1, wall=173
2024-12-12 05:16:02 | INFO | train_inner | epoch 001:   1980 / 312876 loss=10.335, nll_loss=9.518, ppl=733.22, wps=6401.8, ups=17.1, wpb=374.4, bsz=11.2, num_updates=1980, lr=0.0002475, gnorm=2.012, loss_scale=128, train_wall=1, wall=173
2024-12-12 05:16:02 | INFO | train_inner | epoch 001:   1990 / 312876 loss=10.457, nll_loss=9.656, ppl=806.8, wps=6399.9, ups=16.84, wpb=380, bsz=16, num_updates=1990, lr=0.00024875, gnorm=1.887, loss_scale=128, train_wall=1, wall=174
2024-12-12 05:16:03 | INFO | train_inner | epoch 001:   2000 / 312876 loss=10.597, nll_loss=9.825, ppl=907.33, wps=6694.5, ups=17.62, wpb=379.9, bsz=11.9, num_updates=2000, lr=0.00025, gnorm=2.021, loss_scale=128, train_wall=1, wall=174
2024-12-12 05:16:03 | INFO | train_inner | epoch 001:   2010 / 312876 loss=10.368, nll_loss=9.559, ppl=754.07, wps=5820.8, ups=16.92, wpb=344, bsz=13.6, num_updates=2010, lr=0.00025125, gnorm=2.284, loss_scale=128, train_wall=1, wall=175
2024-12-12 05:16:04 | INFO | train_inner | epoch 001:   2020 / 312876 loss=10.506, nll_loss=9.709, ppl=836.89, wps=7047.7, ups=17.02, wpb=414, bsz=19.8, num_updates=2020, lr=0.0002525, gnorm=2.16, loss_scale=128, train_wall=1, wall=176
2024-12-12 05:16:04 | INFO | train_inner | epoch 001:   2030 / 312876 loss=10.456, nll_loss=9.657, ppl=807.07, wps=6862.5, ups=16.74, wpb=410, bsz=15, num_updates=2030, lr=0.00025375, gnorm=1.964, loss_scale=128, train_wall=1, wall=176
2024-12-12 05:16:05 | INFO | train_inner | epoch 001:   2040 / 312876 loss=10.503, nll_loss=9.707, ppl=835.53, wps=5804.4, ups=17.57, wpb=330.4, bsz=11.2, num_updates=2040, lr=0.000255, gnorm=1.943, loss_scale=128, train_wall=1, wall=177
2024-12-12 05:16:06 | INFO | train_inner | epoch 001:   2050 / 312876 loss=10.485, nll_loss=9.698, ppl=830.34, wps=6110, ups=17.56, wpb=348, bsz=12.4, num_updates=2050, lr=0.00025625, gnorm=1.99, loss_scale=128, train_wall=1, wall=177
2024-12-12 05:16:06 | INFO | train_inner | epoch 001:   2060 / 312876 loss=10.575, nll_loss=9.8, ppl=891.49, wps=6235.8, ups=17.88, wpb=348.8, bsz=12, num_updates=2060, lr=0.0002575, gnorm=1.953, loss_scale=128, train_wall=1, wall=178
2024-12-12 05:16:07 | INFO | train_inner | epoch 001:   2070 / 312876 loss=10.737, nll_loss=9.968, ppl=1001.44, wps=7417.7, ups=17.37, wpb=427.1, bsz=12.3, num_updates=2070, lr=0.00025875, gnorm=1.845, loss_scale=128, train_wall=1, wall=178
2024-12-12 05:16:07 | INFO | train_inner | epoch 001:   2080 / 312876 loss=10.455, nll_loss=9.664, ppl=811.27, wps=6236.6, ups=17.42, wpb=358, bsz=12.4, num_updates=2080, lr=0.00026, gnorm=1.934, loss_scale=128, train_wall=1, wall=179
2024-12-12 05:16:08 | INFO | train_inner | epoch 001:   2090 / 312876 loss=10.653, nll_loss=9.892, ppl=950.06, wps=6595.1, ups=17.54, wpb=376, bsz=12.3, num_updates=2090, lr=0.00026125, gnorm=1.967, loss_scale=128, train_wall=1, wall=180
2024-12-12 05:16:08 | INFO | fairseq_cli.train | begin save checkpoint
2024-12-12 05:16:16 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_last.pt (epoch 1 @ 2097 updates, score None) (writing took 7.410095178999654 seconds)
2024-12-12 05:16:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-12-12 05:16:16 | INFO | train | epoch 001 | loss 11.457 | nll_loss 10.79 | ppl 1770.18 | wps 6214.7 | ups 16.44 | wpb 378.1 | bsz 12.7 | num_updates 2097 | lr 0.000262125 | gnorm 2.367 | loss_scale 128 | train_wall 118 | wall 187
2024-12-12 05:16:16 | INFO | fairseq_cli.train | done training in 127.9 seconds
