2024-12-12 06:05:08 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/tmp/wmt14_en_de/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='simple', log_interval=10, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0.05, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2024-12-12 06:05:08 | INFO | fairseq.tasks.translation | [en] dictionary: 40480 types
2024-12-12 06:05:08 | INFO | fairseq.tasks.translation | [de] dictionary: 42720 types
2024-12-12 06:05:08 | INFO | fairseq.data.data_utils | loaded 39414 examples from: /tmp/wmt14_en_de/valid.en-de.en
2024-12-12 06:05:08 | INFO | fairseq.data.data_utils | loaded 39414 examples from: /tmp/wmt14_en_de/valid.en-de.de
2024-12-12 06:05:08 | INFO | fairseq.tasks.translation | /tmp/wmt14_en_de/ valid en-de 39414 examples
2024-12-12 06:05:12 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(40480, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(42720, 1024, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=1024, out_features=42720, bias=False)
  )
)
2024-12-12 06:05:12 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2024-12-12 06:05:12 | INFO | fairseq_cli.train | model: transformer_wmt_en_de_big_t2t (TransformerModel)
2024-12-12 06:05:12 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2024-12-12 06:05:12 | INFO | fairseq_cli.train | num. model params: 305303552 (num. trained: 305303552)
2024-12-12 06:05:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-12 06:05:14 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-SXM2-16GB                    
2024-12-12 06:05:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-12 06:05:14 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-12-12 06:05:14 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None
2024-12-12 06:05:14 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2024-12-12 06:05:14 | INFO | fairseq.trainer | loading train data for epoch 1
2024-12-12 06:05:14 | INFO | fairseq.data.data_utils | loaded 3900502 examples from: /tmp/wmt14_en_de/train.en-de.en
2024-12-12 06:05:14 | INFO | fairseq.data.data_utils | loaded 3900502 examples from: /tmp/wmt14_en_de/train.en-de.de
2024-12-12 06:05:14 | INFO | fairseq.tasks.translation | /tmp/wmt14_en_de/ train en-de 3900502 examples
2024-12-12 06:05:18 | INFO | fairseq.optim.adam | using FusedAdam
2024-12-12 06:05:18 | INFO | fairseq.trainer | begin training epoch 1
2024-12-12 06:05:20 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2024-12-12 06:05:20 | INFO | train_inner | epoch 001:     11 / 31388 loss=16.092, nll_loss=16.09, ppl=69774.3, wps=24514.4, ups=6.65, wpb=3703, bsz=85.6, num_updates=10, lr=1.25e-06, gnorm=4.91, loss_scale=64, train_wall=2, wall=6
2024-12-12 06:05:21 | INFO | train_inner | epoch 001:     21 / 31388 loss=16.028, nll_loss=16.02, ppl=66440.5, wps=27268, ups=7.24, wpb=3766.2, bsz=132, num_updates=20, lr=2.5e-06, gnorm=4.899, loss_scale=64, train_wall=1, wall=8
2024-12-12 06:05:23 | INFO | train_inner | epoch 001:     31 / 31388 loss=15.764, nll_loss=15.727, ppl=54230.7, wps=27341.3, ups=7.31, wpb=3739.8, bsz=163.2, num_updates=30, lr=3.75e-06, gnorm=5.09, loss_scale=64, train_wall=1, wall=9
2024-12-12 06:05:24 | INFO | train_inner | epoch 001:     41 / 31388 loss=15.183, nll_loss=15.08, ppl=34641.6, wps=26956.9, ups=7.32, wpb=3684.8, bsz=133.6, num_updates=40, lr=5e-06, gnorm=4.416, loss_scale=64, train_wall=1, wall=10
2024-12-12 06:05:26 | INFO | train_inner | epoch 001:     51 / 31388 loss=14.53, nll_loss=14.345, ppl=20810.9, wps=24016.5, ups=6.5, wpb=3693.5, bsz=120.8, num_updates=50, lr=6.25e-06, gnorm=2.999, loss_scale=64, train_wall=1, wall=12
2024-12-12 06:05:27 | INFO | train_inner | epoch 001:     61 / 31388 loss=14.063, nll_loss=13.82, ppl=14458.5, wps=27192.7, ups=7.05, wpb=3854.4, bsz=139.2, num_updates=60, lr=7.5e-06, gnorm=2.912, loss_scale=64, train_wall=1, wall=13
2024-12-12 06:05:28 | INFO | train_inner | epoch 001:     71 / 31388 loss=13.87, nll_loss=13.602, ppl=12437.2, wps=27510.8, ups=7.14, wpb=3852.8, bsz=121.6, num_updates=70, lr=8.75e-06, gnorm=1.944, loss_scale=64, train_wall=1, wall=15
2024-12-12 06:05:30 | INFO | train_inner | epoch 001:     81 / 31388 loss=13.705, nll_loss=13.42, ppl=10957.4, wps=26322.3, ups=7.13, wpb=3691.8, bsz=131.2, num_updates=80, lr=1e-05, gnorm=2.034, loss_scale=64, train_wall=1, wall=16
2024-12-12 06:05:31 | INFO | train_inner | epoch 001:     91 / 31388 loss=13.539, nll_loss=13.236, ppl=9645.31, wps=27656.8, ups=7.16, wpb=3864.4, bsz=106.4, num_updates=90, lr=1.125e-05, gnorm=1.541, loss_scale=64, train_wall=1, wall=18
2024-12-12 06:05:33 | INFO | train_inner | epoch 001:    101 / 31388 loss=13.336, nll_loss=13.01, ppl=8246.84, wps=27039.5, ups=7.15, wpb=3780, bsz=133.6, num_updates=100, lr=1.25e-05, gnorm=1.82, loss_scale=64, train_wall=1, wall=19
2024-12-12 06:05:34 | INFO | train_inner | epoch 001:    111 / 31388 loss=13.226, nll_loss=12.886, ppl=7568.69, wps=27398.7, ups=7.23, wpb=3789.7, bsz=112.8, num_updates=110, lr=1.375e-05, gnorm=1.362, loss_scale=64, train_wall=1, wall=20
2024-12-12 06:05:35 | INFO | train_inner | epoch 001:    121 / 31388 loss=13.002, nll_loss=12.639, ppl=6380.61, wps=27019.2, ups=7.07, wpb=3819.2, bsz=138.4, num_updates=120, lr=1.5e-05, gnorm=1.513, loss_scale=64, train_wall=1, wall=22
2024-12-12 06:05:37 | INFO | train_inner | epoch 001:    131 / 31388 loss=12.924, nll_loss=12.546, ppl=5980.98, wps=27454, ups=7.19, wpb=3816, bsz=148, num_updates=130, lr=1.625e-05, gnorm=2.003, loss_scale=64, train_wall=1, wall=23
2024-12-12 06:05:38 | INFO | train_inner | epoch 001:    141 / 31388 loss=12.724, nll_loss=12.328, ppl=5143.23, wps=26178.7, ups=7.28, wpb=3597.3, bsz=151.2, num_updates=140, lr=1.75e-05, gnorm=1.875, loss_scale=64, train_wall=1, wall=25
2024-12-12 06:05:39 | INFO | train_inner | epoch 001:    151 / 31388 loss=12.731, nll_loss=12.331, ppl=5151.52, wps=27635.9, ups=7.29, wpb=3790.3, bsz=117.6, num_updates=150, lr=1.875e-05, gnorm=1.528, loss_scale=64, train_wall=1, wall=26
2024-12-12 06:05:41 | INFO | train_inner | epoch 001:    161 / 31388 loss=12.574, nll_loss=12.16, ppl=4575, wps=27606, ups=7.28, wpb=3793.8, bsz=100.8, num_updates=160, lr=2e-05, gnorm=1.16, loss_scale=64, train_wall=1, wall=27
2024-12-12 06:05:42 | INFO | train_inner | epoch 001:    171 / 31388 loss=12.352, nll_loss=11.905, ppl=3835.58, wps=28293.9, ups=7.13, wpb=3968.1, bsz=148.8, num_updates=170, lr=2.125e-05, gnorm=1.281, loss_scale=64, train_wall=1, wall=29
2024-12-12 06:05:44 | INFO | train_inner | epoch 001:    181 / 31388 loss=12.255, nll_loss=11.797, ppl=3557.32, wps=26598.5, ups=7.18, wpb=3704.7, bsz=116.8, num_updates=180, lr=2.25e-05, gnorm=1.048, loss_scale=64, train_wall=1, wall=30
2024-12-12 06:05:45 | INFO | train_inner | epoch 001:    191 / 31388 loss=12.147, nll_loss=11.669, ppl=3255.33, wps=26041.4, ups=7.03, wpb=3704, bsz=152, num_updates=190, lr=2.375e-05, gnorm=1.493, loss_scale=64, train_wall=1, wall=31
2024-12-12 06:05:46 | INFO | train_inner | epoch 001:    201 / 31388 loss=12.069, nll_loss=11.578, ppl=3056.7, wps=27435.6, ups=7.05, wpb=3888.8, bsz=132.8, num_updates=200, lr=2.5e-05, gnorm=1.08, loss_scale=64, train_wall=1, wall=33
2024-12-12 06:05:48 | INFO | train_inner | epoch 001:    211 / 31388 loss=11.91, nll_loss=11.396, ppl=2694.89, wps=27288.9, ups=7.13, wpb=3825.7, bsz=146.4, num_updates=210, lr=2.625e-05, gnorm=1.043, loss_scale=64, train_wall=1, wall=34
2024-12-12 06:05:49 | INFO | train_inner | epoch 001:    221 / 31388 loss=11.709, nll_loss=11.158, ppl=2285.46, wps=27092.4, ups=7.07, wpb=3832, bsz=211.2, num_updates=220, lr=2.75e-05, gnorm=1.744, loss_scale=64, train_wall=1, wall=36
2024-12-12 06:05:51 | INFO | train_inner | epoch 001:    231 / 31388 loss=11.843, nll_loss=11.311, ppl=2540.62, wps=26743.7, ups=7.36, wpb=3636, bsz=112.8, num_updates=230, lr=2.875e-05, gnorm=1.224, loss_scale=64, train_wall=1, wall=37
2024-12-12 06:05:52 | INFO | train_inner | epoch 001:    241 / 31388 loss=11.745, nll_loss=11.193, ppl=2341.62, wps=27176.8, ups=7.09, wpb=3834.2, bsz=130.4, num_updates=240, lr=3e-05, gnorm=0.97, loss_scale=64, train_wall=1, wall=38
2024-12-12 06:05:53 | INFO | train_inner | epoch 001:    251 / 31388 loss=11.71, nll_loss=11.148, ppl=2269.53, wps=26407.9, ups=7.03, wpb=3756.8, bsz=141.6, num_updates=250, lr=3.125e-05, gnorm=1.155, loss_scale=64, train_wall=1, wall=40
2024-12-12 06:05:55 | INFO | train_inner | epoch 001:    261 / 31388 loss=11.705, nll_loss=11.139, ppl=2254.69, wps=28262.6, ups=7.2, wpb=3925.3, bsz=110.4, num_updates=260, lr=3.25e-05, gnorm=1.194, loss_scale=64, train_wall=1, wall=41
2024-12-12 06:05:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2024-12-12 06:05:56 | INFO | train_inner | epoch 001:    272 / 31388 loss=11.66, nll_loss=11.076, ppl=2159.46, wps=23383.5, ups=6.56, wpb=3565.5, bsz=180.8, num_updates=270, lr=3.375e-05, gnorm=2.68, loss_scale=32, train_wall=1, wall=43
2024-12-12 06:05:58 | INFO | train_inner | epoch 001:    282 / 31388 loss=11.555, nll_loss=10.962, ppl=1994.7, wps=26924.9, ups=7.03, wpb=3832, bsz=116, num_updates=280, lr=3.5e-05, gnorm=1.292, loss_scale=32, train_wall=1, wall=44
2024-12-12 06:05:59 | INFO | train_inner | epoch 001:    292 / 31388 loss=11.536, nll_loss=10.93, ppl=1951.17, wps=27830, ups=7.2, wpb=3863, bsz=118.4, num_updates=290, lr=3.625e-05, gnorm=1.202, loss_scale=32, train_wall=1, wall=46
2024-12-12 06:06:01 | INFO | train_inner | epoch 001:    302 / 31388 loss=11.304, nll_loss=10.665, ppl=1623.66, wps=26822.1, ups=7.08, wpb=3787.8, bsz=184, num_updates=300, lr=3.75e-05, gnorm=2.212, loss_scale=32, train_wall=1, wall=47
2024-12-12 06:06:02 | INFO | train_inner | epoch 001:    312 / 31388 loss=11.535, nll_loss=10.926, ppl=1946.26, wps=26649, ups=7.13, wpb=3735, bsz=91.2, num_updates=310, lr=3.875e-05, gnorm=1.428, loss_scale=32, train_wall=1, wall=48
2024-12-12 06:06:03 | INFO | train_inner | epoch 001:    322 / 31388 loss=11.499, nll_loss=10.881, ppl=1885.4, wps=25797.5, ups=7.08, wpb=3644.8, bsz=110.4, num_updates=320, lr=4e-05, gnorm=1.325, loss_scale=32, train_wall=1, wall=50
2024-12-12 06:06:05 | INFO | train_inner | epoch 001:    332 / 31388 loss=11.476, nll_loss=10.853, ppl=1849.33, wps=26184.2, ups=7.15, wpb=3661.2, bsz=92, num_updates=330, lr=4.125e-05, gnorm=1.185, loss_scale=32, train_wall=1, wall=51
2024-12-12 06:06:06 | INFO | train_inner | epoch 001:    342 / 31388 loss=11.574, nll_loss=10.957, ppl=1987.78, wps=26349.5, ups=7.17, wpb=3675.1, bsz=90.4, num_updates=340, lr=4.25e-05, gnorm=1.53, loss_scale=32, train_wall=1, wall=53
2024-12-12 06:06:08 | INFO | train_inner | epoch 001:    352 / 31388 loss=11.53, nll_loss=10.906, ppl=1919.4, wps=25394.2, ups=7.19, wpb=3531.1, bsz=88.8, num_updates=350, lr=4.375e-05, gnorm=1.794, loss_scale=32, train_wall=1, wall=54
2024-12-12 06:06:09 | INFO | train_inner | epoch 001:    362 / 31388 loss=11.24, nll_loss=10.575, ppl=1525.92, wps=25611.9, ups=6.98, wpb=3669.6, bsz=192, num_updates=360, lr=4.5e-05, gnorm=1.726, loss_scale=32, train_wall=1, wall=55
2024-12-12 06:06:10 | INFO | train_inner | epoch 001:    372 / 31388 loss=11.407, nll_loss=10.763, ppl=1738.13, wps=26568.5, ups=7.16, wpb=3708.8, bsz=125.6, num_updates=370, lr=4.625e-05, gnorm=1.057, loss_scale=32, train_wall=1, wall=57
2024-12-12 06:06:12 | INFO | train_inner | epoch 001:    382 / 31388 loss=11.321, nll_loss=10.665, ppl=1623.63, wps=27159.3, ups=7.1, wpb=3825.8, bsz=120, num_updates=380, lr=4.75e-05, gnorm=1.269, loss_scale=32, train_wall=1, wall=58
2024-12-12 06:06:13 | INFO | train_inner | epoch 001:    392 / 31388 loss=11.281, nll_loss=10.621, ppl=1574.93, wps=26674, ups=7.17, wpb=3719.2, bsz=108.8, num_updates=390, lr=4.875e-05, gnorm=1.122, loss_scale=32, train_wall=1, wall=60
2024-12-12 06:06:15 | INFO | train_inner | epoch 001:    402 / 31388 loss=11.291, nll_loss=10.626, ppl=1580.79, wps=28277.8, ups=7.19, wpb=3935.3, bsz=130.4, num_updates=400, lr=5e-05, gnorm=1.599, loss_scale=32, train_wall=1, wall=61
2024-12-12 06:06:16 | INFO | train_inner | epoch 001:    412 / 31388 loss=11.219, nll_loss=10.541, ppl=1489.99, wps=27572, ups=7.09, wpb=3887, bsz=137.6, num_updates=410, lr=5.125e-05, gnorm=1.715, loss_scale=32, train_wall=1, wall=62
2024-12-12 06:06:17 | INFO | train_inner | epoch 001:    422 / 31388 loss=11.263, nll_loss=10.596, ppl=1548.27, wps=26311.1, ups=7.17, wpb=3668, bsz=122.4, num_updates=420, lr=5.25e-05, gnorm=1.125, loss_scale=32, train_wall=1, wall=64
2024-12-12 06:06:19 | INFO | train_inner | epoch 001:    432 / 31388 loss=11.207, nll_loss=10.532, ppl=1480.86, wps=27696.1, ups=7.3, wpb=3791.4, bsz=136.8, num_updates=430, lr=5.375e-05, gnorm=1.498, loss_scale=32, train_wall=1, wall=65
2024-12-12 06:06:20 | INFO | train_inner | epoch 001:    442 / 31388 loss=10.983, nll_loss=10.277, ppl=1241.06, wps=26564.6, ups=7.2, wpb=3690.6, bsz=180.8, num_updates=440, lr=5.5e-05, gnorm=1.528, loss_scale=32, train_wall=1, wall=67
2024-12-12 06:06:22 | INFO | train_inner | epoch 001:    452 / 31388 loss=11.01, nll_loss=10.308, ppl=1267.37, wps=27794.9, ups=7.06, wpb=3936, bsz=164.8, num_updates=450, lr=5.625e-05, gnorm=1.582, loss_scale=32, train_wall=1, wall=68
2024-12-12 06:06:23 | INFO | train_inner | epoch 001:    462 / 31388 loss=11.104, nll_loss=10.409, ppl=1359.37, wps=27631.6, ups=7.1, wpb=3892, bsz=143.2, num_updates=460, lr=5.75e-05, gnorm=1.182, loss_scale=32, train_wall=1, wall=69
2024-12-12 06:06:24 | INFO | train_inner | epoch 001:    472 / 31388 loss=11.193, nll_loss=10.512, ppl=1460.74, wps=26710.4, ups=7.3, wpb=3658.3, bsz=113.6, num_updates=470, lr=5.875e-05, gnorm=1.117, loss_scale=32, train_wall=1, wall=71
2024-12-12 06:06:26 | INFO | train_inner | epoch 001:    482 / 31388 loss=11.2, nll_loss=10.52, ppl=1468.61, wps=27330.7, ups=7.27, wpb=3760.2, bsz=114.4, num_updates=480, lr=6e-05, gnorm=1.318, loss_scale=32, train_wall=1, wall=72
2024-12-12 06:06:27 | INFO | train_inner | epoch 001:    492 / 31388 loss=11.073, nll_loss=10.374, ppl=1327.27, wps=26392.4, ups=7.19, wpb=3672.8, bsz=123.2, num_updates=490, lr=6.125e-05, gnorm=1.72, loss_scale=32, train_wall=1, wall=74
2024-12-12 06:06:29 | INFO | train_inner | epoch 001:    502 / 31388 loss=11.114, nll_loss=10.425, ppl=1375.15, wps=27963.8, ups=7.38, wpb=3791.1, bsz=110.4, num_updates=500, lr=6.25e-05, gnorm=1.28, loss_scale=32, train_wall=1, wall=75
2024-12-12 06:06:30 | INFO | train_inner | epoch 001:    512 / 31388 loss=11.069, nll_loss=10.369, ppl=1322.17, wps=27125, ups=7.18, wpb=3776.8, bsz=114.4, num_updates=510, lr=6.375e-05, gnorm=1.491, loss_scale=32, train_wall=1, wall=76
2024-12-12 06:06:31 | INFO | train_inner | epoch 001:    522 / 31388 loss=11.082, nll_loss=10.379, ppl=1331.89, wps=28087.7, ups=7.22, wpb=3891.9, bsz=122.4, num_updates=520, lr=6.5e-05, gnorm=1.433, loss_scale=32, train_wall=1, wall=78
2024-12-12 06:06:33 | INFO | train_inner | epoch 001:    532 / 31388 loss=10.97, nll_loss=10.258, ppl=1224.88, wps=27146.2, ups=7.26, wpb=3738.4, bsz=134.4, num_updates=530, lr=6.625e-05, gnorm=1.343, loss_scale=32, train_wall=1, wall=79
2024-12-12 06:06:34 | INFO | train_inner | epoch 001:    542 / 31388 loss=10.883, nll_loss=10.159, ppl=1143.1, wps=27057.9, ups=7.46, wpb=3626.4, bsz=124.8, num_updates=540, lr=6.75e-05, gnorm=1.766, loss_scale=32, train_wall=1, wall=80
2024-12-12 06:06:35 | INFO | train_inner | epoch 001:    552 / 31388 loss=11.047, nll_loss=10.331, ppl=1287.64, wps=27092.4, ups=7.29, wpb=3717.1, bsz=114.4, num_updates=550, lr=6.875e-05, gnorm=1.491, loss_scale=32, train_wall=1, wall=82
2024-12-12 06:06:37 | INFO | train_inner | epoch 001:    562 / 31388 loss=10.987, nll_loss=10.271, ppl=1235.27, wps=26995.4, ups=7.3, wpb=3700, bsz=108.8, num_updates=560, lr=7e-05, gnorm=1.39, loss_scale=32, train_wall=1, wall=83
2024-12-12 06:06:38 | INFO | train_inner | epoch 001:    572 / 31388 loss=10.797, nll_loss=10.061, ppl=1067.86, wps=27137.2, ups=7.27, wpb=3730.4, bsz=116, num_updates=570, lr=7.125e-05, gnorm=1.275, loss_scale=32, train_wall=1, wall=85
2024-12-12 06:06:40 | INFO | train_inner | epoch 001:    582 / 31388 loss=10.798, nll_loss=10.061, ppl=1068.06, wps=27235.5, ups=7.18, wpb=3791, bsz=127.2, num_updates=580, lr=7.25e-05, gnorm=1.447, loss_scale=32, train_wall=1, wall=86
2024-12-12 06:06:41 | INFO | train_inner | epoch 001:    592 / 31388 loss=10.187, nll_loss=9.368, ppl=660.69, wps=27821.7, ups=7.13, wpb=3901.6, bsz=280.8, num_updates=590, lr=7.375e-05, gnorm=1.752, loss_scale=32, train_wall=1, wall=87
2024-12-12 06:06:42 | INFO | train_inner | epoch 001:    602 / 31388 loss=10.886, nll_loss=10.159, ppl=1143.67, wps=27225.5, ups=7.25, wpb=3757.8, bsz=126.4, num_updates=600, lr=7.5e-05, gnorm=1.604, loss_scale=32, train_wall=1, wall=89
2024-12-12 06:06:44 | INFO | train_inner | epoch 001:    612 / 31388 loss=10.792, nll_loss=10.051, ppl=1061.16, wps=25952.3, ups=7.2, wpb=3605.6, bsz=112.8, num_updates=610, lr=7.625e-05, gnorm=1.31, loss_scale=32, train_wall=1, wall=90
2024-12-12 06:06:45 | INFO | train_inner | epoch 001:    622 / 31388 loss=10.716, nll_loss=9.968, ppl=1001.8, wps=26904.2, ups=7.07, wpb=3802.8, bsz=130.4, num_updates=620, lr=7.75e-05, gnorm=1.232, loss_scale=32, train_wall=1, wall=92
2024-12-12 06:06:47 | INFO | train_inner | epoch 001:    632 / 31388 loss=10.717, nll_loss=9.969, ppl=1002.18, wps=25254.5, ups=6.65, wpb=3798.7, bsz=137.6, num_updates=630, lr=7.875e-05, gnorm=1.245, loss_scale=32, train_wall=1, wall=93
2024-12-12 06:06:48 | INFO | train_inner | epoch 001:    642 / 31388 loss=10.769, nll_loss=10.023, ppl=1040.64, wps=27517.7, ups=7.33, wpb=3756.4, bsz=135.2, num_updates=640, lr=8e-05, gnorm=1.486, loss_scale=32, train_wall=1, wall=94
2024-12-12 06:06:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 16.0
2024-12-12 06:06:50 | INFO | train_inner | epoch 001:    653 / 31388 loss=10.715, nll_loss=9.964, ppl=998.81, wps=25250.5, ups=6.63, wpb=3808, bsz=96, num_updates=650, lr=8.125e-05, gnorm=1.222, loss_scale=16, train_wall=1, wall=96
2024-12-12 06:06:51 | INFO | train_inner | epoch 001:    663 / 31388 loss=10.739, nll_loss=9.994, ppl=1020.06, wps=28335.3, ups=7.31, wpb=3876.8, bsz=110.4, num_updates=660, lr=8.25e-05, gnorm=0.977, loss_scale=16, train_wall=1, wall=97
2024-12-12 06:06:52 | INFO | train_inner | epoch 001:    673 / 31388 loss=10.623, nll_loss=9.863, ppl=931.01, wps=27439.7, ups=7.16, wpb=3832.7, bsz=140, num_updates=670, lr=8.375e-05, gnorm=1.042, loss_scale=16, train_wall=1, wall=99
2024-12-12 06:06:54 | INFO | train_inner | epoch 001:    683 / 31388 loss=10.701, nll_loss=9.944, ppl=984.86, wps=26285.5, ups=7.45, wpb=3527.4, bsz=90.4, num_updates=680, lr=8.5e-05, gnorm=1.42, loss_scale=16, train_wall=1, wall=100
2024-12-12 06:06:55 | INFO | train_inner | epoch 001:    693 / 31388 loss=10.558, nll_loss=9.787, ppl=883.48, wps=27462.2, ups=7.06, wpb=3890.8, bsz=147.2, num_updates=690, lr=8.625e-05, gnorm=1.217, loss_scale=16, train_wall=1, wall=101
2024-12-12 06:06:56 | INFO | train_inner | epoch 001:    703 / 31388 loss=10.482, nll_loss=9.702, ppl=833.04, wps=27097.7, ups=7.06, wpb=3835.6, bsz=125.6, num_updates=700, lr=8.75e-05, gnorm=1.153, loss_scale=16, train_wall=1, wall=103
2024-12-12 06:06:58 | INFO | train_inner | epoch 001:    713 / 31388 loss=10.54, nll_loss=9.76, ppl=866.86, wps=26865.4, ups=7.35, wpb=3657.6, bsz=115.2, num_updates=710, lr=8.875e-05, gnorm=1.498, loss_scale=16, train_wall=1, wall=104
2024-12-12 06:06:59 | INFO | train_inner | epoch 001:    723 / 31388 loss=10.537, nll_loss=9.753, ppl=862.78, wps=26480.6, ups=7.35, wpb=3603.5, bsz=141.6, num_updates=720, lr=9e-05, gnorm=1.721, loss_scale=16, train_wall=1, wall=106
2024-12-12 06:07:01 | INFO | train_inner | epoch 001:    733 / 31388 loss=10.416, nll_loss=9.623, ppl=788.41, wps=27047.6, ups=7.46, wpb=3627.4, bsz=156.8, num_updates=730, lr=9.125e-05, gnorm=1.315, loss_scale=16, train_wall=1, wall=107
2024-12-12 06:07:02 | INFO | train_inner | epoch 001:    743 / 31388 loss=10.601, nll_loss=9.827, ppl=908.5, wps=27561.1, ups=7.28, wpb=3786.2, bsz=112.8, num_updates=740, lr=9.25e-05, gnorm=1.335, loss_scale=16, train_wall=1, wall=108
2024-12-12 06:07:03 | INFO | train_inner | epoch 001:    753 / 31388 loss=10.246, nll_loss=9.424, ppl=686.8, wps=26931.7, ups=6.93, wpb=3887.2, bsz=164, num_updates=750, lr=9.375e-05, gnorm=1.738, loss_scale=16, train_wall=1, wall=110
2024-12-12 06:07:05 | INFO | train_inner | epoch 001:    763 / 31388 loss=10.531, nll_loss=9.758, ppl=866.06, wps=26834.7, ups=7.14, wpb=3756.2, bsz=97.6, num_updates=760, lr=9.5e-05, gnorm=1.127, loss_scale=16, train_wall=1, wall=111
2024-12-12 06:07:06 | INFO | train_inner | epoch 001:    773 / 31388 loss=10.388, nll_loss=9.591, ppl=771.25, wps=27256.5, ups=7.15, wpb=3813.5, bsz=107.2, num_updates=770, lr=9.625e-05, gnorm=1.005, loss_scale=16, train_wall=1, wall=113
2024-12-12 06:07:07 | INFO | train_inner | epoch 001:    783 / 31388 loss=10.496, nll_loss=9.71, ppl=837.7, wps=26650.6, ups=7.33, wpb=3635.2, bsz=96.8, num_updates=780, lr=9.75e-05, gnorm=0.946, loss_scale=16, train_wall=1, wall=114
2024-12-12 06:07:09 | INFO | train_inner | epoch 001:    793 / 31388 loss=10.332, nll_loss=9.528, ppl=738.45, wps=27284.3, ups=7.11, wpb=3836.4, bsz=136.8, num_updates=790, lr=9.875e-05, gnorm=1.195, loss_scale=16, train_wall=1, wall=115
2024-12-12 06:07:10 | INFO | train_inner | epoch 001:    803 / 31388 loss=10.132, nll_loss=9.3, ppl=630.2, wps=26968.2, ups=7.17, wpb=3761.6, bsz=138.4, num_updates=800, lr=0.0001, gnorm=0.989, loss_scale=16, train_wall=1, wall=117
2024-12-12 06:07:12 | INFO | train_inner | epoch 001:    813 / 31388 loss=10.403, nll_loss=9.607, ppl=779.71, wps=25976.8, ups=7.31, wpb=3555.1, bsz=122.4, num_updates=810, lr=0.00010125, gnorm=1.255, loss_scale=16, train_wall=1, wall=118
2024-12-12 06:07:13 | INFO | train_inner | epoch 001:    823 / 31388 loss=10.237, nll_loss=9.416, ppl=682.92, wps=26573.6, ups=7.2, wpb=3692.2, bsz=130.4, num_updates=820, lr=0.0001025, gnorm=0.984, loss_scale=16, train_wall=1, wall=119
2024-12-12 06:07:14 | INFO | train_inner | epoch 001:    833 / 31388 loss=10.359, nll_loss=9.557, ppl=753.38, wps=25909.4, ups=7.21, wpb=3593.3, bsz=99.2, num_updates=830, lr=0.00010375, gnorm=1.146, loss_scale=16, train_wall=1, wall=121
2024-12-12 06:07:16 | INFO | train_inner | epoch 001:    843 / 31388 loss=10.199, nll_loss=9.373, ppl=662.86, wps=26577.8, ups=7.05, wpb=3770.4, bsz=116.8, num_updates=840, lr=0.000105, gnorm=1.125, loss_scale=16, train_wall=1, wall=122
2024-12-12 06:07:17 | INFO | train_inner | epoch 001:    853 / 31388 loss=10.317, nll_loss=9.504, ppl=726.1, wps=27374.4, ups=7.12, wpb=3843.7, bsz=130.4, num_updates=850, lr=0.00010625, gnorm=1.234, loss_scale=16, train_wall=1, wall=124
2024-12-12 06:07:19 | INFO | train_inner | epoch 001:    863 / 31388 loss=10.208, nll_loss=9.381, ppl=666.72, wps=27304.7, ups=7.34, wpb=3718.6, bsz=112.8, num_updates=860, lr=0.0001075, gnorm=1.124, loss_scale=16, train_wall=1, wall=125
2024-12-12 06:07:20 | INFO | train_inner | epoch 001:    873 / 31388 loss=10.197, nll_loss=9.37, ppl=661.61, wps=27202.1, ups=7.38, wpb=3683.6, bsz=128.8, num_updates=870, lr=0.00010875, gnorm=1.184, loss_scale=16, train_wall=1, wall=126
2024-12-12 06:07:21 | INFO | train_inner | epoch 001:    883 / 31388 loss=10.145, nll_loss=9.306, ppl=632.86, wps=26654.8, ups=7.16, wpb=3720.3, bsz=109.6, num_updates=880, lr=0.00011, gnorm=1.157, loss_scale=16, train_wall=1, wall=128
2024-12-12 06:07:23 | INFO | train_inner | epoch 001:    893 / 31388 loss=9.972, nll_loss=9.115, ppl=554.32, wps=28071.3, ups=7.37, wpb=3811.2, bsz=168, num_updates=890, lr=0.00011125, gnorm=1.404, loss_scale=16, train_wall=1, wall=129
2024-12-12 06:07:24 | INFO | train_inner | epoch 001:    903 / 31388 loss=10.117, nll_loss=9.279, ppl=621.12, wps=27454.6, ups=7.32, wpb=3750.9, bsz=128, num_updates=900, lr=0.0001125, gnorm=1.159, loss_scale=16, train_wall=1, wall=131
2024-12-12 06:07:26 | INFO | train_inner | epoch 001:    913 / 31388 loss=10.137, nll_loss=9.302, ppl=631.14, wps=26846.3, ups=7.06, wpb=3801, bsz=132, num_updates=910, lr=0.00011375, gnorm=1.339, loss_scale=16, train_wall=1, wall=132
2024-12-12 06:07:27 | INFO | train_inner | epoch 001:    923 / 31388 loss=10.18, nll_loss=9.343, ppl=649.46, wps=26406.3, ups=7.39, wpb=3573.3, bsz=113.6, num_updates=920, lr=0.000115, gnorm=1.004, loss_scale=16, train_wall=1, wall=133
2024-12-12 06:07:28 | INFO | train_inner | epoch 001:    933 / 31388 loss=10.141, nll_loss=9.298, ppl=629.3, wps=27505.2, ups=7.31, wpb=3764.4, bsz=100.8, num_updates=930, lr=0.00011625, gnorm=0.97, loss_scale=16, train_wall=1, wall=135
2024-12-12 06:07:30 | INFO | train_inner | epoch 001:    943 / 31388 loss=9.873, nll_loss=9.002, ppl=512.85, wps=27592.3, ups=7.2, wpb=3830.7, bsz=123.2, num_updates=940, lr=0.0001175, gnorm=0.924, loss_scale=16, train_wall=1, wall=136
2024-12-12 06:07:31 | INFO | train_inner | epoch 001:    953 / 31388 loss=9.996, nll_loss=9.14, ppl=563.99, wps=26690.4, ups=7.09, wpb=3762, bsz=129.6, num_updates=950, lr=0.00011875, gnorm=1.02, loss_scale=16, train_wall=1, wall=137
2024-12-12 06:07:32 | INFO | train_inner | epoch 001:    963 / 31388 loss=9.95, nll_loss=9.086, ppl=543.46, wps=26983.3, ups=7.1, wpb=3798.4, bsz=125.6, num_updates=960, lr=0.00012, gnorm=1.034, loss_scale=16, train_wall=1, wall=139
2024-12-12 06:07:34 | INFO | train_inner | epoch 001:    973 / 31388 loss=9.985, nll_loss=9.126, ppl=558.86, wps=27056.6, ups=7.21, wpb=3750.9, bsz=128.8, num_updates=970, lr=0.00012125, gnorm=1.21, loss_scale=16, train_wall=1, wall=140
2024-12-12 06:07:35 | INFO | train_inner | epoch 001:    983 / 31388 loss=10.089, nll_loss=9.241, ppl=605.14, wps=26670.3, ups=7.26, wpb=3673.1, bsz=100.8, num_updates=980, lr=0.0001225, gnorm=1.351, loss_scale=16, train_wall=1, wall=142
2024-12-12 06:07:37 | INFO | train_inner | epoch 001:    993 / 31388 loss=9.887, nll_loss=9.014, ppl=517.12, wps=27137.2, ups=7.13, wpb=3806.2, bsz=155.2, num_updates=990, lr=0.00012375, gnorm=1.136, loss_scale=16, train_wall=1, wall=143
2024-12-12 06:07:38 | INFO | train_inner | epoch 001:   1003 / 31388 loss=9.982, nll_loss=9.123, ppl=557.57, wps=26320.9, ups=7.38, wpb=3566.4, bsz=113.6, num_updates=1000, lr=0.000125, gnorm=1.136, loss_scale=16, train_wall=1, wall=144
2024-12-12 06:07:39 | INFO | train_inner | epoch 001:   1013 / 31388 loss=10.222, nll_loss=9.389, ppl=670.63, wps=26284.2, ups=7.46, wpb=3521.3, bsz=88.8, num_updates=1010, lr=0.00012625, gnorm=1.153, loss_scale=16, train_wall=1, wall=146
2024-12-12 06:07:41 | INFO | train_inner | epoch 001:   1023 / 31388 loss=9.785, nll_loss=8.889, ppl=474.13, wps=26967.3, ups=7.13, wpb=3783.9, bsz=165.6, num_updates=1020, lr=0.0001275, gnorm=1.467, loss_scale=16, train_wall=1, wall=147
2024-12-12 06:07:42 | INFO | train_inner | epoch 001:   1033 / 31388 loss=9.805, nll_loss=8.918, ppl=483.81, wps=27594.5, ups=7.02, wpb=3929.6, bsz=140, num_updates=1030, lr=0.00012875, gnorm=1.097, loss_scale=16, train_wall=1, wall=149
2024-12-12 06:07:44 | INFO | train_inner | epoch 001:   1043 / 31388 loss=9.702, nll_loss=8.802, ppl=446.21, wps=27359.1, ups=7.05, wpb=3879.8, bsz=140, num_updates=1040, lr=0.00013, gnorm=1.015, loss_scale=16, train_wall=1, wall=150
2024-12-12 06:07:45 | INFO | train_inner | epoch 001:   1053 / 31388 loss=9.804, nll_loss=8.922, ppl=484.95, wps=27243.5, ups=7.25, wpb=3755.2, bsz=127.2, num_updates=1050, lr=0.00013125, gnorm=1.214, loss_scale=16, train_wall=1, wall=151
2024-12-12 06:07:46 | INFO | train_inner | epoch 001:   1063 / 31388 loss=9.623, nll_loss=8.709, ppl=418.38, wps=27680.6, ups=7.3, wpb=3794.4, bsz=143.2, num_updates=1060, lr=0.0001325, gnorm=0.901, loss_scale=16, train_wall=1, wall=153
2024-12-12 06:07:48 | INFO | train_inner | epoch 001:   1073 / 31388 loss=9.828, nll_loss=8.941, ppl=491.6, wps=27444.9, ups=7.27, wpb=3776.8, bsz=129.6, num_updates=1070, lr=0.00013375, gnorm=1.418, loss_scale=16, train_wall=1, wall=154
2024-12-12 06:07:49 | INFO | train_inner | epoch 001:   1083 / 31388 loss=9.723, nll_loss=8.826, ppl=453.97, wps=27041.9, ups=7.32, wpb=3692.1, bsz=154.4, num_updates=1080, lr=0.000135, gnorm=1.13, loss_scale=16, train_wall=1, wall=155
2024-12-12 06:07:50 | INFO | train_inner | epoch 001:   1093 / 31388 loss=9.794, nll_loss=8.904, ppl=479.15, wps=27704.3, ups=7.3, wpb=3794.4, bsz=114.4, num_updates=1090, lr=0.00013625, gnorm=0.987, loss_scale=16, train_wall=1, wall=157
2024-12-12 06:07:52 | INFO | train_inner | epoch 001:   1103 / 31388 loss=9.794, nll_loss=8.901, ppl=478.12, wps=27757.5, ups=7.19, wpb=3858.4, bsz=108.8, num_updates=1100, lr=0.0001375, gnorm=0.877, loss_scale=16, train_wall=1, wall=158
2024-12-12 06:07:53 | INFO | train_inner | epoch 001:   1113 / 31388 loss=9.705, nll_loss=8.803, ppl=446.54, wps=25045.4, ups=7.36, wpb=3401.4, bsz=112, num_updates=1110, lr=0.00013875, gnorm=1.031, loss_scale=16, train_wall=1, wall=160
2024-12-12 06:07:55 | INFO | train_inner | epoch 001:   1123 / 31388 loss=9.621, nll_loss=8.711, ppl=419.08, wps=27277.4, ups=7.12, wpb=3829.4, bsz=139.2, num_updates=1120, lr=0.00014, gnorm=1.54, loss_scale=16, train_wall=1, wall=161
2024-12-12 06:07:56 | INFO | train_inner | epoch 001:   1133 / 31388 loss=9.412, nll_loss=8.461, ppl=352.41, wps=27675.3, ups=7.32, wpb=3779.2, bsz=191.2, num_updates=1130, lr=0.00014125, gnorm=1.302, loss_scale=16, train_wall=1, wall=162
2024-12-12 06:07:57 | INFO | train_inner | epoch 001:   1143 / 31388 loss=9.667, nll_loss=8.768, ppl=436.08, wps=27742.8, ups=7.22, wpb=3844, bsz=134.4, num_updates=1140, lr=0.0001425, gnorm=1.034, loss_scale=16, train_wall=1, wall=164
2024-12-12 06:07:59 | INFO | train_inner | epoch 001:   1153 / 31388 loss=9.584, nll_loss=8.658, ppl=403.9, wps=27875, ups=7.14, wpb=3905.6, bsz=146.4, num_updates=1150, lr=0.00014375, gnorm=1.073, loss_scale=16, train_wall=1, wall=165
2024-12-12 06:08:00 | INFO | train_inner | epoch 001:   1163 / 31388 loss=9.446, nll_loss=8.512, ppl=365.02, wps=26795.9, ups=7.3, wpb=3668.8, bsz=147.2, num_updates=1160, lr=0.000145, gnorm=1.101, loss_scale=16, train_wall=1, wall=167
2024-12-12 06:08:02 | INFO | train_inner | epoch 001:   1173 / 31388 loss=9.643, nll_loss=8.729, ppl=424.39, wps=27356, ups=7.07, wpb=3869.6, bsz=112.8, num_updates=1170, lr=0.00014625, gnorm=0.912, loss_scale=16, train_wall=1, wall=168
2024-12-12 06:08:03 | INFO | train_inner | epoch 001:   1183 / 31388 loss=9.461, nll_loss=8.526, ppl=368.69, wps=26674.9, ups=7.08, wpb=3769.2, bsz=125.6, num_updates=1180, lr=0.0001475, gnorm=0.924, loss_scale=16, train_wall=1, wall=169
2024-12-12 06:08:04 | INFO | train_inner | epoch 001:   1193 / 31388 loss=9.489, nll_loss=8.556, ppl=376.38, wps=26768.4, ups=7.23, wpb=3700.8, bsz=116, num_updates=1190, lr=0.00014875, gnorm=1.069, loss_scale=16, train_wall=1, wall=171
2024-12-12 06:08:06 | INFO | train_inner | epoch 001:   1203 / 31388 loss=9.55, nll_loss=8.624, ppl=394.45, wps=24818.8, ups=6.66, wpb=3725.5, bsz=126.4, num_updates=1200, lr=0.00015, gnorm=1.289, loss_scale=16, train_wall=1, wall=172
2024-12-12 06:08:07 | INFO | train_inner | epoch 001:   1213 / 31388 loss=9.622, nll_loss=8.692, ppl=413.67, wps=27128.8, ups=7.11, wpb=3815, bsz=135.2, num_updates=1210, lr=0.00015125, gnorm=1.243, loss_scale=16, train_wall=1, wall=174
2024-12-12 06:08:09 | INFO | train_inner | epoch 001:   1223 / 31388 loss=9.561, nll_loss=8.638, ppl=398.5, wps=26567.2, ups=7.25, wpb=3665.6, bsz=116, num_updates=1220, lr=0.0001525, gnorm=0.931, loss_scale=16, train_wall=1, wall=175
2024-12-12 06:08:10 | INFO | train_inner | epoch 001:   1233 / 31388 loss=9.663, nll_loss=8.747, ppl=429.65, wps=26147.7, ups=7.4, wpb=3535.3, bsz=100.8, num_updates=1230, lr=0.00015375, gnorm=1.036, loss_scale=16, train_wall=1, wall=176
2024-12-12 06:08:11 | INFO | train_inner | epoch 001:   1243 / 31388 loss=9.51, nll_loss=8.58, ppl=382.69, wps=26993.5, ups=7.13, wpb=3785.6, bsz=108, num_updates=1240, lr=0.000155, gnorm=1.143, loss_scale=16, train_wall=1, wall=178
2024-12-12 06:08:13 | INFO | train_inner | epoch 001:   1253 / 31388 loss=9.683, nll_loss=8.773, ppl=437.6, wps=26735.8, ups=7.3, wpb=3662.3, bsz=90.4, num_updates=1250, lr=0.00015625, gnorm=1.038, loss_scale=16, train_wall=1, wall=179
2024-12-12 06:08:14 | INFO | fairseq_cli.train | begin save checkpoint
2024-12-12 06:08:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_last.pt (epoch 1 @ 1257 updates, score None) (writing took 5.434113551999872 seconds)
2024-12-12 06:08:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-12-12 06:08:19 | INFO | train | epoch 001 | loss 11.018 | nll_loss 10.319 | ppl 1277.63 | wps 26096 | ups 6.95 | wpb 3752.3 | bsz 128.7 | num_updates 1257 | lr 0.000157125 | gnorm 1.446 | loss_scale 16 | train_wall 172 | wall 186
2024-12-12 06:08:19 | INFO | fairseq_cli.train | done training in 181.5 seconds
