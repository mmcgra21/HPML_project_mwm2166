[1/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/general_kernels.cu -o general_kernels.cuda.o 
[2/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/embedding_kernels.cu -o embedding_kernels.cuda.o 
[3/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels.cu -o transform_kernels.cuda.o 
[4/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cublas_wrappers.cu -o cublas_wrappers.cuda.o 
[5/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels_new.cu -o transform_kernels_new.cuda.o 
[6/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/normalize_kernels.cu -o normalize_kernels.cuda.o 
[7/38] c++ -MMD -MF layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/layer.cpp -o layer.o 
[8/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu -o dropout_kernels.cuda.o 
/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1509): warning: variable "thread_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1513): warning: variable "temp_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1545): warning: variable "block_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kRelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kRelu, T=float]" 
(1754): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kGelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kGelu, T=float]" 
(1770): here

[9/38] c++ -MMD -MF node.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/node.cpp -o node.o 
[10/38] c++ -MMD -MF context.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/context.cpp -o context.o 
[11/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels_new.cu -o softmax_kernels_new.cuda.o 
[12/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/crf.cu -o crf.cuda.o 
[13/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels.cu -o softmax_kernels.cuda.o 
[14/38] c++ -MMD -MF manager.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/manager.cpp -o manager.o 
[15/38] c++ -MMD -MF tensor.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/tensor.cpp -o tensor.o 
[16/38] c++ -MMD -MF bias_act_dropout.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/bias_act_dropout.cpp -o bias_act_dropout.o 
[17/38] c++ -MMD -MF bias_dropout_residual.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/bias_dropout_residual.cpp -o bias_dropout_residual.o 
[18/38] c++ -MMD -MF linear.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/linear.cpp -o linear.o 
[19/38] c++ -MMD -MF strided_batch_gemm.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/strided_batch_gemm.cpp -o strided_batch_gemm.o 
[20/38] c++ -MMD -MF dropout.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/dropout.cpp -o dropout.o 
[21/38] c++ -MMD -MF bias_add_transform_20314.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/bias_add_transform_20314.cpp -o bias_add_transform_20314.o 
[22/38] c++ -MMD -MF layer_normalize.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/layer_normalize.cpp -o layer_normalize.o 
[23/38] c++ -MMD -MF softmax.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/softmax.cpp -o softmax.o 
[24/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cross_entropy.cu -o cross_entropy.cuda.o 
[25/38] c++ -MMD -MF launch_concat3_dim1.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/launch_concat3_dim1.cpp -o launch_concat3_dim1.o 
[26/38] c++ -MMD -MF transform_0213.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/transform_0213.cpp -o transform_0213.o 
[27/38] c++ -MMD -MF crf.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/crf.cpp -o crf.o 
[28/38] c++ -MMD -MF feed_forward_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/feed_forward_layer.cpp -o feed_forward_layer.o 
[29/38] c++ -MMD -MF transformer_encoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/transformer_encoder_layer.cpp -o transformer_encoder_layer.o 
[30/38] c++ -MMD -MF encdec_kv_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/encdec_kv_layer.cpp -o encdec_kv_layer.o 
[31/38] c++ -MMD -MF crf_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/crf_layer.cpp -o crf_layer.o 
[32/38] c++ -MMD -MF multihead_attention_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/multihead_attention_layer.cpp -o multihead_attention_layer.o 
[33/38] c++ -MMD -MF transformer_decoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/transformer_decoder_layer.cpp -o transformer_decoder_layer.o 
[34/38] c++ -MMD -MF dec_enc_attention_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/dec_enc_attention_layer.cpp -o dec_enc_attention_layer.o 
[35/38] c++ -MMD -MF dec_self_attention_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/dec_self_attention_layer.cpp -o dec_self_attention_layer.o 
[36/38] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cuda_util.cu -o cuda_util.cuda.o 
[37/38] c++ -MMD -MF pybind_layer_new.o.d -DTORCH_EXTENSION_NAME=lightseq_layers_new -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops_new/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/lsflow/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers_new/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DPYBIND_LAYER -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_layer_new.cpp -o pybind_layer_new.o 
[38/38] c++ cublas_wrappers.cuda.o transform_kernels.cuda.o transform_kernels_new.cuda.o dropout_kernels.cuda.o normalize_kernels.cuda.o softmax_kernels.cuda.o softmax_kernels_new.cuda.o general_kernels.cuda.o cuda_util.cuda.o embedding_kernels.cuda.o cross_entropy.cuda.o crf.cuda.o context.o layer.o manager.o node.o tensor.o bias_act_dropout.o bias_dropout_residual.o linear.o layer_normalize.o strided_batch_gemm.o bias_add_transform_20314.o dropout.o softmax.o launch_concat3_dim1.o transform_0213.o crf.o feed_forward_layer.o multihead_attention_layer.o transformer_encoder_layer.o dec_self_attention_layer.o encdec_kv_layer.o dec_enc_attention_layer.o transformer_decoder_layer.o crf_layer.o pybind_layer_new.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o lightseq_layers_new.so
Time to load lightseq_layers_new op: 50.965938329696655 seconds
[1/18] c++ -MMD -MF cublas_wrappers.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cublas_wrappers.cpp -o cublas_wrappers.o 
[2/18] c++ -MMD -MF cublas_algo_map.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cublas_algo_map.cpp -o cublas_algo_map.o 
[3/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/general_kernels.cu -o general_kernels.cuda.o 
[4/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/quantize_kernels.cu -o quantize_kernels.cuda.o 
[5/18] c++ -MMD -MF cross_entropy_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/cross_entropy_layer.cpp -o cross_entropy_layer.o 
[6/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels.cu -o transform_kernels.cuda.o 
[7/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/embedding_kernels.cu -o embedding_kernels.cuda.o 
[8/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/normalize_kernels.cu -o normalize_kernels.cuda.o 
[9/18] c++ -MMD -MF quant_linear_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/quant_linear_layer.cpp -o quant_linear_layer.o 
[10/18] c++ -MMD -MF transformer_embedding_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/transformer_embedding_layer.cpp -o transformer_embedding_layer.o 
[11/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu -o dropout_kernels.cuda.o 
/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1509): warning: variable "thread_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1513): warning: variable "temp_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1545): warning: variable "block_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kRelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kRelu, T=float]" 
(1754): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kGelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kGelu, T=float]" 
(1770): here

[12/18] c++ -MMD -MF transformer_encoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/transformer_encoder_layer.cpp -o transformer_encoder_layer.o 
[13/18] c++ -MMD -MF transformer_decoder_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/transformer_decoder_layer.cpp -o transformer_decoder_layer.o 
[14/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels.cu -o softmax_kernels.cuda.o 
[15/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cross_entropy.cu -o cross_entropy.cuda.o 
[16/18] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cuda_util.cu -o cuda_util.cuda.o 
[17/18] c++ -MMD -MF pybind_layer.o.d -DTORCH_EXTENSION_NAME=lightseq_layers -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_layer.cpp -o pybind_layer.o 
[18/18] c++ cublas_algo_map.o cublas_wrappers.o quantize_kernels.cuda.o transform_kernels.cuda.o dropout_kernels.cuda.o normalize_kernels.cuda.o softmax_kernels.cuda.o general_kernels.cuda.o cuda_util.cuda.o embedding_kernels.cuda.o cross_entropy.cuda.o cross_entropy_layer.o quant_linear_layer.o transformer_encoder_layer.o transformer_decoder_layer.o transformer_embedding_layer.o pybind_layer.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o lightseq_layers.so
Time to load lightseq_layers op: 39.16814708709717 seconds
Time to load lightseq_layers op: 0.039173126220703125 seconds
Time to load lightseq_layers op: 0.03919529914855957 seconds
Time to load lightseq_layers op: 0.03614497184753418 seconds
[1/14] c++ -MMD -MF gemm_test.o.d -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/gemm_test.cpp -o gemm_test.o 
[2/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/general_kernels.cu -o general_kernels.cuda.o 
[3/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels_new.cu -o transform_kernels_new.cuda.o 
[4/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/transform_kernels.cu -o transform_kernels.cuda.o 
[5/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/embedding_kernels.cu -o embedding_kernels.cuda.o 
[6/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/normalize_kernels.cu -o normalize_kernels.cuda.o 
[7/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/crf.cu -o crf.cuda.o 
[8/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/quantize_kernels.cu -o quantize_kernels.cuda.o 
[9/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu -o dropout_kernels.cuda.o 
/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1509): warning: variable "thread_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1513): warning: variable "temp_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1545): warning: variable "block_cmax_out_grad" was declared but never referenced
          detected during instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const int8_t *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t, __nv_bool) [with act_type=ActivationType::kRelu, T=float]" 
(1601): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kRelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kRelu, T=float]" 
(1754): here

/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/dropout_kernels.cu(1667): warning: variable "cmax_in_val" was declared but never referenced
          detected during:
            instantiation of "void ls_quant_dropout_act_bias_bwd_kernel<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, float, int) [with act_type=ActivationType::kGelu, T=float]" 
(1751): here
            instantiation of "void launch_ls_quant_dropout_act_bias_bwd<act_type,T>(T *, T *, T *, T *, const T *, const T *, const uint8_t *, const uint8_t *, const T *, const T *, const uint8_t *, int, int, float, cudaStream_t) [with act_type=ActivationType::kGelu, T=float]" 
(1770): here

[10/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels_new.cu -o softmax_kernels_new.cuda.o 
[11/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/softmax_kernels.cu -o softmax_kernels.cuda.o 
[12/14] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/cuda_util.cu -o cuda_util.cuda.o 
[13/14] c++ -MMD -MF pybind_kernel.o.d -DTORCH_EXTENSION_NAME=lightseq_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_kernel.cpp -o pybind_kernel.o 
[14/14] c++ gemm_test.o cuda_util.cuda.o transform_kernels.cuda.o transform_kernels_new.cuda.o softmax_kernels.cuda.o softmax_kernels_new.cuda.o general_kernels.cuda.o normalize_kernels.cuda.o dropout_kernels.cuda.o embedding_kernels.cuda.o quantize_kernels.cuda.o crf.cuda.o pybind_kernel.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o lightseq_kernels.so
Time to load lightseq_kernels op: 38.694653272628784 seconds
2024-12-12 05:40:25 | INFO | fairseq_cli.train | Namespace(GCQ_quantile=0.99, activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='ls_transformer_wmt_en_de_big_t2t', attention_dropout=0.1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='ls_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/tmp/wmt14_en_de/', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layerdrop=0, decoder_layers=18, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, enable_GCQ=False, enable_quant=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layerdrop=0, encoder_layers=18, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=True, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', ignore_prefix_size=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format='simple', log_interval=10, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=300, max_target_positions=300, max_tokens=1024, max_tokens_valid=1024, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, min_lr=-1.0, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='ls_adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_mode='qat', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0.05, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, use_torch_layer=False, user_dir='/opt/conda/lib/python3.7/site-packages/lightseq/training/cli/fs_modules', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none')
2024-12-12 05:40:25 | INFO | fairseq.tasks.translation | [en] dictionary: 40480 types
2024-12-12 05:40:25 | INFO | fairseq.tasks.translation | [de] dictionary: 42720 types
2024-12-12 05:40:25 | INFO | fairseq.data.data_utils | loaded 39414 examples from: /tmp/wmt14_en_de/valid.en-de.en
2024-12-12 05:40:25 | INFO | fairseq.data.data_utils | loaded 39414 examples from: /tmp/wmt14_en_de/valid.en-de.de
2024-12-12 05:40:25 | INFO | fairseq.tasks.translation | /tmp/wmt14_en_de/ valid en-de 39414 examples
Initial Context, status_type: Training
Embedding layer #0 is created with date type [half].
Embedding layer #1 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #0 allocate shared memory size: 10158080
Encoder layer #0 allocate shared quant memory size: 9437184
Encoder layer #0 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #1 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #2 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #3 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #4 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #5 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #6 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #7 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #8 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #9 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #10 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #11 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #12 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #13 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #14 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #15 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #16 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Encoder layer #17 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #0 allocate shared memory size: 10158080
Decoder layer #0 allocate shared quant memory size: 9437184
Decoder layer #0 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #1 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #2 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #3 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #4 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #5 is created with date type [half].
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 0}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 1}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 2}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 3}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 4}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 5}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 6}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 7}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 8}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 9}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 10}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 11}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 12}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 13}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 14}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 15}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 16}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'activation_fn': 'relu', 'layer_id': 17}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 0}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 1}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 2}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 3}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 4}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 5}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 6}Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #6 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #7 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #8 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #9 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #10 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #11 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #12 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #13 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #14 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #15 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #16 is created with date type [half].
Get igemm config from /home/mwm2166/.lightseq/igemm_configs/
[WARNING] /home/mwm2166/.lightseq/igemm_configs/ is not found; using default GEMM algo
Decoder layer #17 is created with date type [half].
QuantLinearLayer is created with date type [half].
CrossEntropyLayer is created with date type [half].

Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 7}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 8}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 9}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 10}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 11}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 12}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 13}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 14}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 15}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 16}
Lightseq Transformer config is  {'max_batch_tokens': 1024, 'max_seq_len': 300, 'hidden_size': 1024, 'intermediate_size': 4096, 'nhead': 16, 'attn_prob_dropout_ratio': 0.1, 'activation_dropout_ratio': 0.1, 'hidden_dropout_ratio': 0.3, 'pre_layer_norm': True, 'fp16': True, 'local_rank': 0, 'nlayer': 18, 'activation_fn': 'relu', 'has_cross_attn': True, 'layer_id': 17}
Time to load lightseq_layers op: 0.04350781440734863 seconds
Time to load lightseq_layers op: 0.0386502742767334 seconds
2024-12-12 05:40:30 | INFO | fairseq_cli.train | LSTransformerModel(
  (encoder): LSTransformerEncoder(
    (embed_tokens): LSTransformerEmbeddingLayer()
    (layers): ModuleList(
      (0): LSTransformerEncoderLayer()
      (1): LSTransformerEncoderLayer()
      (2): LSTransformerEncoderLayer()
      (3): LSTransformerEncoderLayer()
      (4): LSTransformerEncoderLayer()
      (5): LSTransformerEncoderLayer()
      (6): LSTransformerEncoderLayer()
      (7): LSTransformerEncoderLayer()
      (8): LSTransformerEncoderLayer()
      (9): LSTransformerEncoderLayer()
      (10): LSTransformerEncoderLayer()
      (11): LSTransformerEncoderLayer()
      (12): LSTransformerEncoderLayer()
      (13): LSTransformerEncoderLayer()
      (14): LSTransformerEncoderLayer()
      (15): LSTransformerEncoderLayer()
      (16): LSTransformerEncoderLayer()
      (17): LSTransformerEncoderLayer()
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): LSTransformerDecoder(
    (embed_tokens): LSTransformerEmbeddingLayer()
    (layers): ModuleList(
      (0): LSFSTransformerDecoderLayer()
      (1): LSFSTransformerDecoderLayer()
      (2): LSFSTransformerDecoderLayer()
      (3): LSFSTransformerDecoderLayer()
      (4): LSFSTransformerDecoderLayer()
      (5): LSFSTransformerDecoderLayer()
      (6): LSFSTransformerDecoderLayer()
      (7): LSFSTransformerDecoderLayer()
      (8): LSFSTransformerDecoderLayer()
      (9): LSFSTransformerDecoderLayer()
      (10): LSFSTransformerDecoderLayer()
      (11): LSFSTransformerDecoderLayer()
      (12): LSFSTransformerDecoderLayer()
      (13): LSFSTransformerDecoderLayer()
      (14): LSFSTransformerDecoderLayer()
      (15): LSFSTransformerDecoderLayer()
      (16): LSFSTransformerDecoderLayer()
      (17): LSFSTransformerDecoderLayer()
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (output_projection): LSQuantLinearLayer()
  )
)
2024-12-12 05:40:30 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2024-12-12 05:40:30 | INFO | fairseq_cli.train | model: ls_transformer_wmt_en_de_big_t2t (LSTransformerModel)
2024-12-12 05:40:30 | INFO | fairseq_cli.train | criterion: ls_label_smoothed_cross_entropy (LSLabelSmoothedCrossEntropyCriterion)
2024-12-12 05:40:30 | INFO | fairseq_cli.train | num. model params: 614273674 (num. trained: 614273674)
2024-12-12 05:40:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-12 05:40:30 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.782 GB ; name = Tesla V100-SXM2-16GB                    
2024-12-12 05:40:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2024-12-12 05:40:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2024-12-12 05:40:30 | INFO | fairseq_cli.train | max tokens per GPU = 1024 and max sentences per GPU = None
2024-12-12 05:40:30 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/checkpoint_last.pt
2024-12-12 05:40:30 | INFO | fairseq.trainer | loading train data for epoch 1
2024-12-12 05:40:30 | INFO | fairseq.data.data_utils | loaded 3900502 examples from: /tmp/wmt14_en_de/train.en-de.en
2024-12-12 05:40:31 | INFO | fairseq.data.data_utils | loaded 3900502 examples from: /tmp/wmt14_en_de/train.en-de.de
2024-12-12 05:40:31 | INFO | fairseq.tasks.translation | /tmp/wmt14_en_de/ train en-de 3900502 examples
2024-12-12 05:40:34 | INFO | fs_modules.ls_adam | using LightSeq Adam
[1/3] c++ -MMD -MF pybind_adam.o.d -DTORCH_EXTENSION_NAME=adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/pybind/pybind_adam.cpp -o pybind_adam.o 
[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/ops/includes -I/opt/conda/lib/python3.7/site-packages/lightseq/csrc/layers/includes -isystem /opt/conda/lib/python3.7/site-packages/torch/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.7/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.7/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/lib/python3.7/site-packages/lightseq/csrc/kernels/fused_adam_kernel.cu -o fused_adam_kernel.cuda.o 
[3/3] c++ fused_adam_kernel.cuda.o pybind_adam.o -shared -L/opt/conda/lib/python3.7/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o adam.so
Time to load adam op: 55.264806032180786 seconds
2024-12-12 05:41:30 | INFO | fairseq.trainer | begin training epoch 1
TransformerEmbeddingLayer #0 bind weights and grads.
TransformerEncoderLayer #0 bind weights and grads.
TransformerEncoderLayer #1 bind weights and grads.
TransformerEncoderLayer #2 bind weights and grads.
TransformerEncoderLayer #3 bind weights and grads.
TransformerEncoderLayer #4 bind weights and grads.
TransformerEncoderLayer #5 bind weights and grads.
TransformerEncoderLayer #6 bind weights and grads.
TransformerEncoderLayer #7 bind weights and grads.
TransformerEncoderLayer #8 bind weights and grads.
TransformerEncoderLayer #9 bind weights and grads.
TransformerEncoderLayer #10 bind weights and grads.
TransformerEncoderLayer #11 bind weights and grads.
TransformerEncoderLayer #12 bind weights and grads.
TransformerEncoderLayer #13 bind weights and grads.
TransformerEncoderLayer #14 bind weights and grads.
TransformerEncoderLayer #15 bind weights and grads.
TransformerEncoderLayer #16 bind weights and grads.
TransformerEncoderLayer #17 bind weights and grads.
TransformerEmbeddingLayer #1 bind weights and grads.
TransformerDecoderLayer #0 bind weights and grads.
Decoder layer #0 allocate encdec_kv memory
TransformerDecoderLayer #1 bind weights and grads.
TransformerDecoderLayer #2 bind weights and grads.
TransformerDecoderLayer #3 bind weights and grads.
TransformerDecoderLayer #4 bind weights and grads.
TransformerDecoderLayer #5 bind weights and grads.
TransformerDecoderLayer #6 bind weights and grads.
TransformerDecoderLayer #7 bind weights and grads.
TransformerDecoderLayer #8 bind weights and grads.
TransformerDecoderLayer #9 bind weights and grads.
TransformerDecoderLayer #10 bind weights and grads.
TransformerDecoderLayer #11 bind weights and grads.
TransformerDecoderLayer #12 bind weights and grads.
TransformerDecoderLayer #13 bind weights and grads.
TransformerDecoderLayer #14 bind weights and grads.
TransformerDecoderLayer #15 bind weights and grads.
TransformerDecoderLayer #16 bind weights and grads.
TransformerDecoderLayer #17 bind weights and grads.
2024-12-12 05:41:31 | INFO | train_inner | epoch 001:     10 / 145082 loss=19.187, nll_loss=19.195, ppl=600198, wps=7829.8, ups=9.64, wpb=808.8, bsz=27.2, num_updates=10, lr=1.25e-06, gnorm=18.736, loss_scale=128, train_wall=1, wall=61
2024-12-12 05:41:32 | INFO | train_inner | epoch 001:     20 / 145082 loss=18.613, nll_loss=18.616, ppl=401705, wps=7864.4, ups=10.31, wpb=763.1, bsz=23.2, num_updates=20, lr=2.5e-06, gnorm=17.912, loss_scale=128, train_wall=1, wall=61
2024-12-12 05:41:33 | INFO | train_inner | epoch 001:     30 / 145082 loss=16.783, nll_loss=16.774, ppl=112074, wps=8179.5, ups=9.76, wpb=837.9, bsz=40.8, num_updates=30, lr=3.75e-06, gnorm=9.21, loss_scale=128, train_wall=1, wall=62
2024-12-12 05:41:34 | INFO | train_inner | epoch 001:     40 / 145082 loss=15.934, nll_loss=15.891, ppl=60784.7, wps=7829.4, ups=10.18, wpb=768.8, bsz=24, num_updates=40, lr=5e-06, gnorm=5.156, loss_scale=128, train_wall=1, wall=63
2024-12-12 05:41:35 | INFO | train_inner | epoch 001:     50 / 145082 loss=15.234, nll_loss=15.118, ppl=35555.6, wps=8253.9, ups=9.82, wpb=840.3, bsz=28.8, num_updates=50, lr=6.25e-06, gnorm=4.149, loss_scale=128, train_wall=1, wall=64
2024-12-12 05:41:36 | INFO | train_inner | epoch 001:     60 / 145082 loss=14.826, nll_loss=14.656, ppl=25820.5, wps=7975.8, ups=9.91, wpb=804.8, bsz=29.6, num_updates=60, lr=7.5e-06, gnorm=3.219, loss_scale=128, train_wall=1, wall=66
2024-12-12 05:41:37 | INFO | train_inner | epoch 001:     70 / 145082 loss=14.362, nll_loss=14.138, ppl=18023.1, wps=7399.4, ups=10.55, wpb=701.3, bsz=20, num_updates=70, lr=8.75e-06, gnorm=2.863, loss_scale=128, train_wall=1, wall=66
2024-12-12 05:41:38 | INFO | train_inner | epoch 001:     80 / 145082 loss=14.113, nll_loss=13.865, ppl=14924.4, wps=7235.2, ups=10.62, wpb=681.6, bsz=22.4, num_updates=80, lr=1e-05, gnorm=2.608, loss_scale=128, train_wall=1, wall=67
2024-12-12 05:41:39 | INFO | train_inner | epoch 001:     90 / 145082 loss=13.838, nll_loss=13.561, ppl=12083.2, wps=7727.3, ups=10.3, wpb=750.4, bsz=22.4, num_updates=90, lr=1.125e-05, gnorm=2.2, loss_scale=128, train_wall=1, wall=68
2024-12-12 05:41:40 | INFO | train_inner | epoch 001:    100 / 145082 loss=13.626, nll_loss=13.327, ppl=10279.1, wps=7852, ups=10.02, wpb=783.7, bsz=27.2, num_updates=100, lr=1.25e-05, gnorm=2.111, loss_scale=128, train_wall=1, wall=69
2024-12-12 05:41:41 | INFO | train_inner | epoch 001:    110 / 145082 loss=13.46, nll_loss=13.144, ppl=9049.53, wps=7965.1, ups=9.82, wpb=811.2, bsz=24.8, num_updates=110, lr=1.375e-05, gnorm=1.884, loss_scale=128, train_wall=1, wall=70
2024-12-12 05:41:42 | INFO | train_inner | epoch 001:    120 / 145082 loss=13.254, nll_loss=12.914, ppl=7719.93, wps=8002.6, ups=9.75, wpb=820.8, bsz=22.4, num_updates=120, lr=1.5e-05, gnorm=1.774, loss_scale=128, train_wall=1, wall=71
2024-12-12 05:41:43 | INFO | train_inner | epoch 001:    130 / 145082 loss=13.078, nll_loss=12.718, ppl=6738.99, wps=8019.6, ups=9.95, wpb=805.6, bsz=33.6, num_updates=130, lr=1.625e-05, gnorm=1.891, loss_scale=128, train_wall=1, wall=72
2024-12-12 05:41:44 | INFO | train_inner | epoch 001:    140 / 145082 loss=12.958, nll_loss=12.583, ppl=6135.62, wps=8448, ups=10.09, wpb=837.6, bsz=24, num_updates=140, lr=1.75e-05, gnorm=1.912, loss_scale=128, train_wall=1, wall=73
2024-12-12 05:41:45 | INFO | train_inner | epoch 001:    150 / 145082 loss=12.904, nll_loss=12.524, ppl=5889.52, wps=7609.8, ups=10.18, wpb=747.2, bsz=25.6, num_updates=150, lr=1.875e-05, gnorm=1.92, loss_scale=128, train_wall=1, wall=74
2024-12-12 05:41:46 | INFO | train_inner | epoch 001:    160 / 145082 loss=12.738, nll_loss=12.334, ppl=5162.43, wps=7491.8, ups=10.25, wpb=731.2, bsz=21.6, num_updates=160, lr=2e-05, gnorm=2.066, loss_scale=128, train_wall=1, wall=75
2024-12-12 05:41:47 | INFO | train_inner | epoch 001:    170 / 145082 loss=12.614, nll_loss=12.193, ppl=4681.62, wps=8143.3, ups=9.8, wpb=831.2, bsz=28.8, num_updates=170, lr=2.125e-05, gnorm=1.827, loss_scale=128, train_wall=1, wall=76
2024-12-12 05:41:48 | INFO | train_inner | epoch 001:    180 / 145082 loss=12.428, nll_loss=11.985, ppl=4052.79, wps=8254.7, ups=9.8, wpb=842.4, bsz=34.4, num_updates=180, lr=2.25e-05, gnorm=1.891, loss_scale=128, train_wall=1, wall=77
2024-12-12 05:41:49 | INFO | train_inner | epoch 001:    190 / 145082 loss=12.395, nll_loss=11.938, ppl=3922.7, wps=7903.5, ups=10.43, wpb=757.6, bsz=22.4, num_updates=190, lr=2.375e-05, gnorm=1.743, loss_scale=128, train_wall=1, wall=78
2024-12-12 05:41:50 | INFO | train_inner | epoch 001:    200 / 145082 loss=12.39, nll_loss=11.93, ppl=3901.14, wps=8006, ups=10.06, wpb=796, bsz=25.6, num_updates=200, lr=2.5e-05, gnorm=1.542, loss_scale=128, train_wall=1, wall=79
2024-12-12 05:41:51 | INFO | train_inner | epoch 001:    210 / 145082 loss=12.214, nll_loss=11.724, ppl=3383.19, wps=7738.4, ups=10.11, wpb=765.6, bsz=24.8, num_updates=210, lr=2.625e-05, gnorm=1.915, loss_scale=128, train_wall=1, wall=80
2024-12-12 05:41:52 | INFO | train_inner | epoch 001:    220 / 145082 loss=12.187, nll_loss=11.69, ppl=3303.99, wps=8416.3, ups=9.92, wpb=848.8, bsz=25.6, num_updates=220, lr=2.75e-05, gnorm=1.835, loss_scale=128, train_wall=1, wall=81
2024-12-12 05:41:53 | INFO | train_inner | epoch 001:    230 / 145082 loss=12.206, nll_loss=11.71, ppl=3348.98, wps=8320.2, ups=10.14, wpb=820.5, bsz=22.4, num_updates=230, lr=2.875e-05, gnorm=1.481, loss_scale=128, train_wall=1, wall=82
2024-12-12 05:41:54 | INFO | train_inner | epoch 001:    240 / 145082 loss=12.006, nll_loss=11.477, ppl=2850.72, wps=8495.3, ups=9.79, wpb=868, bsz=32.8, num_updates=240, lr=3e-05, gnorm=1.755, loss_scale=128, train_wall=1, wall=83
2024-12-12 05:41:55 | INFO | train_inner | epoch 001:    250 / 145082 loss=11.881, nll_loss=11.336, ppl=2584.97, wps=7896.7, ups=10.29, wpb=767.2, bsz=24.8, num_updates=250, lr=3.125e-05, gnorm=1.535, loss_scale=128, train_wall=1, wall=84
2024-12-12 05:41:56 | INFO | train_inner | epoch 001:    260 / 145082 loss=11.924, nll_loss=11.373, ppl=2652.26, wps=8027.9, ups=9.94, wpb=808, bsz=25.6, num_updates=260, lr=3.25e-05, gnorm=1.552, loss_scale=128, train_wall=1, wall=85
2024-12-12 05:41:57 | INFO | train_inner | epoch 001:    270 / 145082 loss=11.847, nll_loss=11.282, ppl=2490.82, wps=7977.8, ups=10.12, wpb=788, bsz=28, num_updates=270, lr=3.375e-05, gnorm=1.727, loss_scale=128, train_wall=1, wall=86
2024-12-12 05:41:58 | INFO | train_inner | epoch 001:    280 / 145082 loss=11.82, nll_loss=11.247, ppl=2430.86, wps=7916.9, ups=10.09, wpb=784.5, bsz=23.1, num_updates=280, lr=3.5e-05, gnorm=1.861, loss_scale=128, train_wall=1, wall=87
2024-12-12 05:41:59 | INFO | train_inner | epoch 001:    290 / 145082 loss=11.725, nll_loss=11.137, ppl=2252.4, wps=7858.9, ups=9.89, wpb=794.4, bsz=25.6, num_updates=290, lr=3.625e-05, gnorm=1.742, loss_scale=128, train_wall=1, wall=88
2024-12-12 05:42:00 | INFO | train_inner | epoch 001:    300 / 145082 loss=11.758, nll_loss=11.173, ppl=2308.39, wps=7711.4, ups=9.91, wpb=777.8, bsz=21.6, num_updates=300, lr=3.75e-05, gnorm=1.573, loss_scale=128, train_wall=1, wall=89
2024-12-12 05:42:01 | INFO | train_inner | epoch 001:    310 / 145082 loss=11.821, nll_loss=11.238, ppl=2414.77, wps=7785.2, ups=10.07, wpb=772.8, bsz=35.2, num_updates=310, lr=3.875e-05, gnorm=2.412, loss_scale=128, train_wall=1, wall=90
2024-12-12 05:42:02 | INFO | train_inner | epoch 001:    320 / 145082 loss=11.834, nll_loss=11.25, ppl=2435.52, wps=7729.1, ups=9.93, wpb=778.4, bsz=18.4, num_updates=320, lr=4e-05, gnorm=1.653, loss_scale=128, train_wall=1, wall=91
2024-12-12 05:42:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2024-12-12 05:42:03 | INFO | train_inner | epoch 001:    331 / 145082 loss=11.789, nll_loss=11.197, ppl=2348.03, wps=7303.4, ups=9.47, wpb=771.2, bsz=20, num_updates=330, lr=4.125e-05, gnorm=1.676, loss_scale=64, train_wall=1, wall=92
2024-12-12 05:42:04 | INFO | train_inner | epoch 001:    341 / 145082 loss=11.832, nll_loss=11.21, ppl=2368.84, wps=7746.4, ups=10.09, wpb=768, bsz=30.2, num_updates=340, lr=4.25e-05, gnorm=3.556, loss_scale=64, train_wall=1, wall=93
2024-12-12 05:42:05 | INFO | train_inner | epoch 001:    351 / 145082 loss=11.769, nll_loss=11.171, ppl=2305.25, wps=8338.1, ups=9.95, wpb=837.6, bsz=24, num_updates=350, lr=4.375e-05, gnorm=1.454, loss_scale=64, train_wall=1, wall=94
2024-12-12 05:42:06 | INFO | train_inner | epoch 001:    361 / 145082 loss=11.639, nll_loss=11.013, ppl=2066.46, wps=7983.1, ups=9.94, wpb=803.3, bsz=29.6, num_updates=360, lr=4.5e-05, gnorm=2.094, loss_scale=64, train_wall=1, wall=95
2024-12-12 05:42:07 | INFO | train_inner | epoch 001:    371 / 145082 loss=11.706, nll_loss=11.085, ppl=2172.73, wps=8239.5, ups=9.74, wpb=845.6, bsz=32.8, num_updates=370, lr=4.625e-05, gnorm=1.536, loss_scale=64, train_wall=1, wall=96
2024-12-12 05:42:08 | INFO | train_inner | epoch 001:    381 / 145082 loss=11.681, nll_loss=11.058, ppl=2132.01, wps=7752.1, ups=10.21, wpb=759.4, bsz=27.2, num_updates=380, lr=4.75e-05, gnorm=1.926, loss_scale=64, train_wall=1, wall=97
2024-12-12 05:42:09 | INFO | train_inner | epoch 001:    391 / 145082 loss=11.55, nll_loss=10.914, ppl=1929.21, wps=7623.3, ups=10.3, wpb=740, bsz=27.2, num_updates=390, lr=4.875e-05, gnorm=1.732, loss_scale=64, train_wall=1, wall=98
2024-12-12 05:42:10 | INFO | train_inner | epoch 001:    401 / 145082 loss=11.645, nll_loss=11.015, ppl=2069.14, wps=8178.3, ups=9.81, wpb=833.6, bsz=24, num_updates=400, lr=5e-05, gnorm=1.548, loss_scale=64, train_wall=1, wall=99
2024-12-12 05:42:11 | INFO | train_inner | epoch 001:    411 / 145082 loss=11.575, nll_loss=10.935, ppl=1958.19, wps=7837.9, ups=10.55, wpb=743.2, bsz=20, num_updates=410, lr=5.125e-05, gnorm=1.553, loss_scale=64, train_wall=1, wall=100
2024-12-12 05:42:12 | INFO | train_inner | epoch 001:    421 / 145082 loss=11.476, nll_loss=10.824, ppl=1813.19, wps=7924.7, ups=9.75, wpb=812.8, bsz=35.2, num_updates=420, lr=5.25e-05, gnorm=1.872, loss_scale=64, train_wall=1, wall=101
2024-12-12 05:42:13 | INFO | train_inner | epoch 001:    431 / 145082 loss=11.475, nll_loss=10.816, ppl=1803.19, wps=8315.6, ups=9.91, wpb=839.2, bsz=25.6, num_updates=430, lr=5.375e-05, gnorm=1.475, loss_scale=64, train_wall=1, wall=102
2024-12-12 05:42:14 | INFO | train_inner | epoch 001:    441 / 145082 loss=11.491, nll_loss=10.84, ppl=1832.75, wps=7080.6, ups=10.69, wpb=662.4, bsz=16.8, num_updates=440, lr=5.5e-05, gnorm=1.705, loss_scale=64, train_wall=1, wall=103
2024-12-12 05:42:15 | INFO | train_inner | epoch 001:    451 / 145082 loss=11.515, nll_loss=10.86, ppl=1858.66, wps=8824.2, ups=9.68, wpb=912, bsz=26.4, num_updates=450, lr=5.625e-05, gnorm=1.635, loss_scale=64, train_wall=1, wall=104
2024-12-12 05:42:16 | INFO | train_inner | epoch 001:    461 / 145082 loss=11.345, nll_loss=10.671, ppl=1629.88, wps=7706.9, ups=10.05, wpb=767.2, bsz=31.2, num_updates=460, lr=5.75e-05, gnorm=1.764, loss_scale=64, train_wall=1, wall=105
2024-12-12 05:42:17 | INFO | train_inner | epoch 001:    471 / 145082 loss=11.411, nll_loss=10.745, ppl=1715.7, wps=7793.7, ups=10.17, wpb=766, bsz=24, num_updates=470, lr=5.875e-05, gnorm=1.678, loss_scale=64, train_wall=1, wall=106
2024-12-12 05:42:18 | INFO | train_inner | epoch 001:    481 / 145082 loss=11.369, nll_loss=10.696, ppl=1658.96, wps=8703.7, ups=9.49, wpb=916.8, bsz=37.6, num_updates=480, lr=6e-05, gnorm=1.903, loss_scale=64, train_wall=1, wall=107
2024-12-12 05:42:19 | INFO | train_inner | epoch 001:    491 / 145082 loss=11.418, nll_loss=10.752, ppl=1724.85, wps=8474, ups=10.06, wpb=842.4, bsz=32, num_updates=490, lr=6.125e-05, gnorm=1.902, loss_scale=64, train_wall=1, wall=108
2024-12-12 05:42:20 | INFO | train_inner | epoch 001:    501 / 145082 loss=11.364, nll_loss=10.69, ppl=1652.34, wps=7898.6, ups=10.07, wpb=784.2, bsz=30.4, num_updates=500, lr=6.25e-05, gnorm=2.061, loss_scale=64, train_wall=1, wall=109
2024-12-12 05:42:21 | INFO | train_inner | epoch 001:    511 / 145082 loss=11.458, nll_loss=10.8, ppl=1782.63, wps=6428.6, ups=8.46, wpb=760, bsz=17.6, num_updates=510, lr=6.375e-05, gnorm=1.568, loss_scale=64, train_wall=1, wall=111
2024-12-12 05:42:22 | INFO | train_inner | epoch 001:    521 / 145082 loss=11.499, nll_loss=10.843, ppl=1837.27, wps=7675.1, ups=10.12, wpb=758.6, bsz=20.8, num_updates=520, lr=6.5e-05, gnorm=1.788, loss_scale=64, train_wall=1, wall=112
2024-12-12 05:42:23 | INFO | train_inner | epoch 001:    531 / 145082 loss=11.226, nll_loss=10.533, ppl=1481.29, wps=8663, ups=9.74, wpb=889.6, bsz=35.2, num_updates=530, lr=6.625e-05, gnorm=1.381, loss_scale=64, train_wall=1, wall=113
2024-12-12 05:42:24 | INFO | train_inner | epoch 001:    541 / 145082 loss=11.207, nll_loss=10.509, ppl=1457.31, wps=7872.5, ups=9.68, wpb=813.6, bsz=32.8, num_updates=540, lr=6.75e-05, gnorm=1.792, loss_scale=64, train_wall=1, wall=114
2024-12-12 05:42:25 | INFO | train_inner | epoch 001:    551 / 145082 loss=11.182, nll_loss=10.486, ppl=1434.46, wps=8158.5, ups=9.65, wpb=845.6, bsz=35.2, num_updates=550, lr=6.875e-05, gnorm=1.861, loss_scale=64, train_wall=1, wall=115
2024-12-12 05:42:26 | INFO | train_inner | epoch 001:    561 / 145082 loss=11.208, nll_loss=10.51, ppl=1457.78, wps=8350.9, ups=9.73, wpb=858.4, bsz=30.4, num_updates=560, lr=7e-05, gnorm=1.745, loss_scale=64, train_wall=1, wall=116
2024-12-12 05:42:27 | INFO | train_inner | epoch 001:    571 / 145082 loss=11.252, nll_loss=10.564, ppl=1513.51, wps=8234.5, ups=9.62, wpb=856.3, bsz=38, num_updates=570, lr=7.125e-05, gnorm=1.841, loss_scale=64, train_wall=1, wall=117
2024-12-12 05:42:28 | INFO | train_inner | epoch 001:    581 / 145082 loss=11.498, nll_loss=10.83, ppl=1820.25, wps=8453.3, ups=10.04, wpb=841.6, bsz=27.2, num_updates=580, lr=7.25e-05, gnorm=2.854, loss_scale=64, train_wall=1, wall=118
2024-12-12 05:42:29 | INFO | train_inner | epoch 001:    591 / 145082 loss=11.454, nll_loss=10.791, ppl=1772.25, wps=8432, ups=9.65, wpb=874.2, bsz=32.8, num_updates=590, lr=7.375e-05, gnorm=1.77, loss_scale=64, train_wall=1, wall=119
2024-12-12 05:42:30 | INFO | train_inner | epoch 001:    601 / 145082 loss=11.336, nll_loss=10.647, ppl=1603.51, wps=8348.4, ups=9.86, wpb=846.4, bsz=28, num_updates=600, lr=7.5e-05, gnorm=1.81, loss_scale=64, train_wall=1, wall=120
2024-12-12 05:42:31 | INFO | train_inner | epoch 001:    611 / 145082 loss=11.33, nll_loss=10.662, ppl=1620.47, wps=8145.6, ups=10.08, wpb=808.1, bsz=28.8, num_updates=610, lr=7.625e-05, gnorm=1.639, loss_scale=64, train_wall=1, wall=121
2024-12-12 05:42:32 | INFO | train_inner | epoch 001:    621 / 145082 loss=11.123, nll_loss=10.419, ppl=1369.2, wps=7765.3, ups=9.86, wpb=787.2, bsz=31.2, num_updates=620, lr=7.75e-05, gnorm=1.514, loss_scale=64, train_wall=1, wall=122
2024-12-12 05:42:33 | INFO | train_inner | epoch 001:    631 / 145082 loss=11.256, nll_loss=10.566, ppl=1516.38, wps=7606.7, ups=10.36, wpb=734.4, bsz=19.2, num_updates=630, lr=7.875e-05, gnorm=1.644, loss_scale=64, train_wall=1, wall=123
2024-12-12 05:42:34 | INFO | train_inner | epoch 001:    641 / 145082 loss=11.405, nll_loss=10.743, ppl=1713.54, wps=7563.6, ups=10.32, wpb=732.8, bsz=23.2, num_updates=640, lr=8e-05, gnorm=1.47, loss_scale=64, train_wall=1, wall=124
2024-12-12 05:42:35 | INFO | train_inner | epoch 001:    651 / 145082 loss=11.043, nll_loss=10.327, ppl=1284.88, wps=7971.4, ups=9.68, wpb=823.2, bsz=28, num_updates=650, lr=8.125e-05, gnorm=1.474, loss_scale=64, train_wall=1, wall=125
2024-12-12 05:42:36 | INFO | train_inner | epoch 001:    661 / 145082 loss=11.175, nll_loss=10.474, ppl=1421.99, wps=7872.1, ups=9.99, wpb=788, bsz=20.8, num_updates=660, lr=8.25e-05, gnorm=1.578, loss_scale=64, train_wall=1, wall=126
2024-12-12 05:42:37 | INFO | train_inner | epoch 001:    671 / 145082 loss=11.239, nll_loss=10.55, ppl=1498.94, wps=7985.5, ups=10.25, wpb=779.2, bsz=22.4, num_updates=670, lr=8.375e-05, gnorm=1.621, loss_scale=64, train_wall=1, wall=127
2024-12-12 05:42:38 | INFO | train_inner | epoch 001:    681 / 145082 loss=11.198, nll_loss=10.504, ppl=1451.86, wps=8255.8, ups=10.12, wpb=816, bsz=25.6, num_updates=680, lr=8.5e-05, gnorm=1.662, loss_scale=64, train_wall=1, wall=128
2024-12-12 05:42:39 | INFO | train_inner | epoch 001:    691 / 145082 loss=11.157, nll_loss=10.459, ppl=1407.14, wps=8338.3, ups=10.03, wpb=831.2, bsz=28, num_updates=690, lr=8.625e-05, gnorm=1.803, loss_scale=64, train_wall=1, wall=129
2024-12-12 05:42:40 | INFO | train_inner | epoch 001:    701 / 145082 loss=11.121, nll_loss=10.417, ppl=1366.76, wps=7747.8, ups=10.16, wpb=762.7, bsz=20.8, num_updates=700, lr=8.75e-05, gnorm=1.775, loss_scale=64, train_wall=1, wall=130
2024-12-12 05:42:41 | INFO | train_inner | epoch 001:    711 / 145082 loss=11.182, nll_loss=10.483, ppl=1430.71, wps=8050.2, ups=9.88, wpb=815.2, bsz=26.4, num_updates=710, lr=8.875e-05, gnorm=1.545, loss_scale=64, train_wall=1, wall=131
2024-12-12 05:42:42 | INFO | train_inner | epoch 001:    721 / 145082 loss=11.074, nll_loss=10.364, ppl=1317.98, wps=8043.2, ups=10.09, wpb=796.8, bsz=26.4, num_updates=720, lr=9e-05, gnorm=1.648, loss_scale=64, train_wall=1, wall=132
2024-12-12 05:42:43 | INFO | train_inner | epoch 001:    731 / 145082 loss=11.047, nll_loss=10.329, ppl=1286.16, wps=8405, ups=9.79, wpb=858.4, bsz=31.2, num_updates=730, lr=9.125e-05, gnorm=1.501, loss_scale=64, train_wall=1, wall=133
2024-12-12 05:42:44 | INFO | train_inner | epoch 001:    741 / 145082 loss=10.907, nll_loss=10.176, ppl=1156.87, wps=7856.5, ups=9.9, wpb=793.6, bsz=30.4, num_updates=740, lr=9.25e-05, gnorm=1.553, loss_scale=64, train_wall=1, wall=134
2024-12-12 05:42:45 | INFO | train_inner | epoch 001:    751 / 145082 loss=10.952, nll_loss=10.216, ppl=1189.11, wps=8562.8, ups=9.39, wpb=912.1, bsz=32, num_updates=750, lr=9.375e-05, gnorm=2.12, loss_scale=64, train_wall=1, wall=135
2024-12-12 05:42:46 | INFO | train_inner | epoch 001:    761 / 145082 loss=11.038, nll_loss=10.325, ppl=1282.43, wps=7953.4, ups=9.99, wpb=796.3, bsz=24, num_updates=760, lr=9.5e-05, gnorm=1.593, loss_scale=64, train_wall=1, wall=136
2024-12-12 05:42:47 | INFO | train_inner | epoch 001:    771 / 145082 loss=11.14, nll_loss=10.434, ppl=1383.4, wps=7512.5, ups=10.36, wpb=725.4, bsz=20, num_updates=770, lr=9.625e-05, gnorm=1.559, loss_scale=64, train_wall=1, wall=137
2024-12-12 05:42:48 | INFO | train_inner | epoch 001:    781 / 145082 loss=11.053, nll_loss=10.34, ppl=1296.43, wps=7662.8, ups=10.07, wpb=760.6, bsz=27.2, num_updates=780, lr=9.75e-05, gnorm=1.934, loss_scale=64, train_wall=1, wall=138
2024-12-12 05:42:49 | INFO | train_inner | epoch 001:    791 / 145082 loss=10.837, nll_loss=10.094, ppl=1092.88, wps=8501.5, ups=9.49, wpb=895.9, bsz=35, num_updates=790, lr=9.875e-05, gnorm=1.576, loss_scale=64, train_wall=1, wall=139
2024-12-12 05:42:50 | INFO | train_inner | epoch 001:    801 / 145082 loss=11.231, nll_loss=10.534, ppl=1482.39, wps=7981.3, ups=10.11, wpb=789.6, bsz=23.2, num_updates=800, lr=0.0001, gnorm=1.596, loss_scale=64, train_wall=1, wall=140
2024-12-12 05:42:51 | INFO | train_inner | epoch 001:    811 / 145082 loss=10.981, nll_loss=10.258, ppl=1224.45, wps=7961, ups=9.76, wpb=816, bsz=29.6, num_updates=810, lr=0.00010125, gnorm=1.494, loss_scale=64, train_wall=1, wall=141
2024-12-12 05:42:52 | INFO | train_inner | epoch 001:    821 / 145082 loss=11.037, nll_loss=10.327, ppl=1284.64, wps=8162.5, ups=10.23, wpb=797.6, bsz=21.6, num_updates=820, lr=0.0001025, gnorm=1.512, loss_scale=64, train_wall=1, wall=142
2024-12-12 05:42:53 | INFO | train_inner | epoch 001:    831 / 145082 loss=10.908, nll_loss=10.17, ppl=1152.37, wps=8135.1, ups=10, wpb=813.1, bsz=29.6, num_updates=830, lr=0.00010375, gnorm=1.655, loss_scale=64, train_wall=1, wall=143
2024-12-12 05:42:54 | INFO | train_inner | epoch 001:    841 / 145082 loss=11.049, nll_loss=10.342, ppl=1298.2, wps=7419.1, ups=10.32, wpb=719.2, bsz=23.2, num_updates=840, lr=0.000105, gnorm=1.636, loss_scale=64, train_wall=1, wall=144
2024-12-12 05:42:55 | INFO | train_inner | epoch 001:    851 / 145082 loss=10.995, nll_loss=10.275, ppl=1239.37, wps=8408.5, ups=9.87, wpb=852, bsz=24, num_updates=850, lr=0.00010625, gnorm=1.458, loss_scale=64, train_wall=1, wall=145
2024-12-12 05:42:56 | INFO | train_inner | epoch 001:    861 / 145082 loss=10.859, nll_loss=10.123, ppl=1114.87, wps=8456.6, ups=9.9, wpb=854.4, bsz=27.2, num_updates=860, lr=0.0001075, gnorm=1.534, loss_scale=64, train_wall=1, wall=146
2024-12-12 05:42:57 | INFO | train_inner | epoch 001:    871 / 145082 loss=10.933, nll_loss=10.196, ppl=1173.09, wps=8018.9, ups=10.05, wpb=798.1, bsz=28.8, num_updates=870, lr=0.00010875, gnorm=1.888, loss_scale=64, train_wall=1, wall=147
2024-12-12 05:42:58 | INFO | train_inner | epoch 001:    881 / 145082 loss=11.064, nll_loss=10.354, ppl=1308.67, wps=7761.6, ups=10.09, wpb=769.6, bsz=26.4, num_updates=880, lr=0.00011, gnorm=2.082, loss_scale=64, train_wall=1, wall=148
2024-12-12 05:42:59 | INFO | train_inner | epoch 001:    891 / 145082 loss=10.891, nll_loss=10.155, ppl=1139.99, wps=7878.8, ups=10.14, wpb=776.8, bsz=24, num_updates=890, lr=0.00011125, gnorm=1.582, loss_scale=64, train_wall=1, wall=149
2024-12-12 05:43:00 | INFO | train_inner | epoch 001:    901 / 145082 loss=10.856, nll_loss=10.113, ppl=1107.15, wps=9007.6, ups=9.44, wpb=954.4, bsz=32.8, num_updates=900, lr=0.0001125, gnorm=1.437, loss_scale=64, train_wall=1, wall=150
2024-12-12 05:43:01 | INFO | train_inner | epoch 001:    911 / 145082 loss=10.674, nll_loss=9.912, ppl=963.5, wps=7586.6, ups=10.13, wpb=749.1, bsz=34.4, num_updates=910, lr=0.00011375, gnorm=1.841, loss_scale=64, train_wall=1, wall=151
2024-12-12 05:43:02 | INFO | train_inner | epoch 001:    921 / 145082 loss=10.847, nll_loss=10.101, ppl=1098.5, wps=7926.3, ups=9.85, wpb=804.8, bsz=25.6, num_updates=920, lr=0.000115, gnorm=1.575, loss_scale=64, train_wall=1, wall=152
2024-12-12 05:43:03 | INFO | train_inner | epoch 001:    931 / 145082 loss=11.227, nll_loss=10.536, ppl=1484.86, wps=7832.2, ups=10.03, wpb=780.8, bsz=19, num_updates=930, lr=0.00011625, gnorm=1.792, loss_scale=64, train_wall=1, wall=153
2024-12-12 05:43:04 | INFO | train_inner | epoch 001:    941 / 145082 loss=10.717, nll_loss=9.946, ppl=986.58, wps=7886.4, ups=9.63, wpb=819.2, bsz=28, num_updates=940, lr=0.0001175, gnorm=1.661, loss_scale=64, train_wall=1, wall=154
2024-12-12 05:43:05 | INFO | train_inner | epoch 001:    951 / 145082 loss=10.726, nll_loss=9.976, ppl=1006.95, wps=8166.3, ups=10.03, wpb=814.1, bsz=33.6, num_updates=950, lr=0.00011875, gnorm=2.204, loss_scale=64, train_wall=1, wall=155
2024-12-12 05:43:06 | INFO | train_inner | epoch 001:    961 / 145082 loss=10.847, nll_loss=10.096, ppl=1094.49, wps=7900.9, ups=10.15, wpb=778.4, bsz=30.4, num_updates=960, lr=0.00012, gnorm=1.744, loss_scale=64, train_wall=1, wall=156
2024-12-12 05:43:07 | INFO | train_inner | epoch 001:    971 / 145082 loss=10.883, nll_loss=10.146, ppl=1133.14, wps=7993.9, ups=9.87, wpb=809.6, bsz=27.2, num_updates=970, lr=0.00012125, gnorm=1.714, loss_scale=64, train_wall=1, wall=157
2024-12-12 05:43:08 | INFO | train_inner | epoch 001:    981 / 145082 loss=10.863, nll_loss=10.126, ppl=1117.45, wps=8064.6, ups=9.88, wpb=816, bsz=30.4, num_updates=980, lr=0.0001225, gnorm=1.41, loss_scale=64, train_wall=1, wall=158
2024-12-12 05:43:09 | INFO | train_inner | epoch 001:    991 / 145082 loss=10.993, nll_loss=10.275, ppl=1238.76, wps=8082, ups=10.12, wpb=798.3, bsz=25.2, num_updates=990, lr=0.00012375, gnorm=1.687, loss_scale=64, train_wall=1, wall=159
2024-12-12 05:43:10 | INFO | train_inner | epoch 001:   1001 / 145082 loss=10.851, nll_loss=10.114, ppl=1107.88, wps=8014, ups=9.95, wpb=805.6, bsz=30.4, num_updates=1000, lr=0.000125, gnorm=1.519, loss_scale=64, train_wall=1, wall=160
2024-12-12 05:43:11 | INFO | train_inner | epoch 001:   1011 / 145082 loss=10.812, nll_loss=10.068, ppl=1073.13, wps=7761, ups=10.19, wpb=761.6, bsz=19.2, num_updates=1010, lr=0.00012625, gnorm=1.603, loss_scale=64, train_wall=1, wall=161
2024-12-12 05:43:12 | INFO | train_inner | epoch 001:   1021 / 145082 loss=10.753, nll_loss=9.999, ppl=1023.23, wps=8016.6, ups=9.87, wpb=812.2, bsz=28, num_updates=1020, lr=0.0001275, gnorm=1.602, loss_scale=64, train_wall=1, wall=162
2024-12-12 05:43:13 | INFO | train_inner | epoch 001:   1031 / 145082 loss=10.879, nll_loss=10.139, ppl=1127.87, wps=8094.1, ups=10.18, wpb=795.2, bsz=23.2, num_updates=1030, lr=0.00012875, gnorm=1.42, loss_scale=64, train_wall=1, wall=163
2024-12-12 05:43:14 | INFO | train_inner | epoch 001:   1041 / 145082 loss=10.815, nll_loss=10.077, ppl=1079.9, wps=8223, ups=9.92, wpb=829.3, bsz=30.8, num_updates=1040, lr=0.00013, gnorm=1.499, loss_scale=64, train_wall=1, wall=164
2024-12-12 05:43:15 | INFO | train_inner | epoch 001:   1051 / 145082 loss=10.677, nll_loss=9.915, ppl=965.7, wps=7782.7, ups=10.53, wpb=739.2, bsz=24, num_updates=1050, lr=0.00013125, gnorm=1.674, loss_scale=64, train_wall=1, wall=165
2024-12-12 05:43:16 | INFO | train_inner | epoch 001:   1061 / 145082 loss=10.64, nll_loss=9.87, ppl=936.02, wps=8675, ups=9.73, wpb=892, bsz=32, num_updates=1060, lr=0.0001325, gnorm=1.72, loss_scale=64, train_wall=1, wall=166
2024-12-12 05:43:17 | INFO | train_inner | epoch 001:   1071 / 145082 loss=10.639, nll_loss=9.868, ppl=934.49, wps=8475.8, ups=9.83, wpb=862.4, bsz=27.2, num_updates=1070, lr=0.00013375, gnorm=1.71, loss_scale=64, train_wall=1, wall=167
2024-12-12 05:43:18 | INFO | train_inner | epoch 001:   1081 / 145082 loss=10.845, nll_loss=10.106, ppl=1101.84, wps=8309.5, ups=9.72, wpb=855.2, bsz=28.8, num_updates=1080, lr=0.000135, gnorm=1.869, loss_scale=64, train_wall=1, wall=168
2024-12-12 05:43:19 | INFO | train_inner | epoch 001:   1091 / 145082 loss=10.918, nll_loss=10.187, ppl=1165.94, wps=8199, ups=10.29, wpb=796.8, bsz=17.6, num_updates=1090, lr=0.00013625, gnorm=1.533, loss_scale=64, train_wall=1, wall=169
2024-12-12 05:43:20 | INFO | train_inner | epoch 001:   1101 / 145082 loss=10.663, nll_loss=9.891, ppl=949.54, wps=8490.5, ups=9.84, wpb=863.2, bsz=28, num_updates=1100, lr=0.0001375, gnorm=1.513, loss_scale=64, train_wall=1, wall=170
2024-12-12 05:43:21 | INFO | train_inner | epoch 001:   1111 / 145082 loss=10.924, nll_loss=10.192, ppl=1169.58, wps=8281.8, ups=10.17, wpb=814.2, bsz=20.8, num_updates=1110, lr=0.00013875, gnorm=1.464, loss_scale=64, train_wall=1, wall=171
2024-12-12 05:43:22 | INFO | train_inner | epoch 001:   1121 / 145082 loss=10.631, nll_loss=9.868, ppl=934.69, wps=8508, ups=9.96, wpb=854.1, bsz=28.8, num_updates=1120, lr=0.00014, gnorm=1.361, loss_scale=64, train_wall=1, wall=172
2024-12-12 05:43:23 | INFO | train_inner | epoch 001:   1131 / 145082 loss=10.625, nll_loss=9.857, ppl=927.57, wps=8217.8, ups=9.7, wpb=847.2, bsz=32.8, num_updates=1130, lr=0.00014125, gnorm=1.534, loss_scale=64, train_wall=1, wall=173
2024-12-12 05:43:24 | INFO | train_inner | epoch 001:   1141 / 145082 loss=10.825, nll_loss=10.082, ppl=1083.84, wps=7813, ups=9.97, wpb=783.4, bsz=22.2, num_updates=1140, lr=0.0001425, gnorm=1.79, loss_scale=64, train_wall=1, wall=174
2024-12-12 05:43:25 | INFO | train_inner | epoch 001:   1151 / 145082 loss=10.671, nll_loss=9.909, ppl=961.15, wps=7876.9, ups=9.95, wpb=792, bsz=20.8, num_updates=1150, lr=0.00014375, gnorm=1.493, loss_scale=64, train_wall=1, wall=175
2024-12-12 05:43:26 | INFO | train_inner | epoch 001:   1161 / 145082 loss=10.388, nll_loss=9.581, ppl=765.77, wps=8133.8, ups=10.01, wpb=812.8, bsz=30.4, num_updates=1160, lr=0.000145, gnorm=1.69, loss_scale=64, train_wall=1, wall=176
2024-12-12 05:43:27 | INFO | train_inner | epoch 001:   1171 / 145082 loss=10.542, nll_loss=9.766, ppl=870.5, wps=8407, ups=9.9, wpb=848.8, bsz=34.4, num_updates=1170, lr=0.00014625, gnorm=1.368, loss_scale=64, train_wall=1, wall=177
2024-12-12 05:43:28 | INFO | train_inner | epoch 001:   1181 / 145082 loss=10.59, nll_loss=9.81, ppl=897.81, wps=8382.7, ups=9.78, wpb=856.8, bsz=27.2, num_updates=1180, lr=0.0001475, gnorm=1.41, loss_scale=64, train_wall=1, wall=178
2024-12-12 05:43:29 | INFO | train_inner | epoch 001:   1191 / 145082 loss=10.575, nll_loss=9.802, ppl=892.54, wps=8466.6, ups=9.86, wpb=858.5, bsz=25.6, num_updates=1190, lr=0.00014875, gnorm=1.306, loss_scale=64, train_wall=1, wall=179
2024-12-12 05:43:30 | INFO | train_inner | epoch 001:   1201 / 145082 loss=10.532, nll_loss=9.746, ppl=858.81, wps=7503.1, ups=10.47, wpb=716.8, bsz=23.2, num_updates=1200, lr=0.00015, gnorm=1.533, loss_scale=64, train_wall=1, wall=180
2024-12-12 05:43:30 | INFO | fairseq_cli.train | begin save checkpoint
2024-12-12 05:43:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/checkpoint_last.pt (epoch 1 @ 1202 updates, score None) (writing took 10.79239502099972 seconds)
2024-12-12 05:43:41 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2024-12-12 05:43:41 | INFO | train | epoch 001 | loss 11.684 | nll_loss 11.063 | ppl 2139.01 | wps 7360.5 | ups 9.15 | wpb 804.8 | bsz 26.9 | num_updates 1202 | lr 0.00015025 | gnorm 2.136 | loss_scale 64 | train_wall 119 | wall 191
2024-12-12 05:43:41 | INFO | fairseq_cli.train | done training in 131.6 seconds
